{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c45a2a-7e2d-4397-91da-bd45474f72a0",
   "metadata": {},
   "source": [
    "## Download VQVAE from DALLE\n",
    "| testing usage\n",
    "```python\n",
    "enc = encoder\n",
    "dec = decoder\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1d2f0-21d1-4c06-b4ea-8b5708205a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os, sys\n",
    "import requests\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from dall_e import map_pixels, unmap_pixels, load_model\n",
    "from IPython.display import display, display_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b5f24-3259-4577-ab94-14a1ef133439",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_size = 256\n",
    "\n",
    "def download_image(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return PIL.Image.open(io.BytesIO(resp.content))\n",
    "\n",
    "def preprocess(img):\n",
    "    s = min(img.size)\n",
    "    \n",
    "    if s < target_image_size:\n",
    "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "        \n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "    return map_pixels(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf70f0-04fc-4cb3-a77f-080bf1153187",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "enc = load_model(\"https://cdn.openai.com/dall-e/encoder.pkl\", device)\n",
    "# dec = load_model(\"https://cdn.openai.com/dall-e/decoder.pkl\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc9970-5f05-4122-9573-05b2d9e73b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = preprocess(download_image('https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iKIWgaiJUtss/v2/1000x-1.jpg'))\n",
    "display_markdown('Original image:')\n",
    "display(T.ToPILImage(mode='RGB')(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58a5d8-1977-4f13-afb3-5acd66599547",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f1f3a-fe2f-4a7b-be56-21b5b4f6bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageCodebook_len = enc.vocab_size\n",
    "imageCodebook_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416d1ec-388b-4a36-8888-3df61e03b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to(device)\n",
    "\n",
    "z_logits = enc(x)\n",
    "z = torch.argmax(z_logits, axis=1)\n",
    "\n",
    "z_ = F.one_hot(z, num_classes=imageCodebook_len).permute(0, 3, 1, 2).float()\n",
    "\n",
    "x_stats = dec(z_).float()\n",
    "x_rec = unmap_pixels(torch.sigmoid(x_stats[:, :3]))\n",
    "x_rec = T.ToPILImage(mode='RGB')(x_rec[0])\n",
    "\n",
    "display_markdown('Reconstructed image:')\n",
    "display(x_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ed84e-76ef-45d7-857e-468af5615adf",
   "metadata": {},
   "source": [
    "## LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5946a13-280f-4e87-9f4c-f5b4da820235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f9b9f-1e39-47dd-9c98-187c65360792",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"gpt2-large\"\n",
    "# checkpoint = \"princeton-nlp/Sheared-LLaMA-1.3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec616d11-4241-4006-8ff9-461dad3914ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm_model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3079ff28-1756-422e-b4e2-33212971c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# llm_tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)\n",
    "textVocab_len = len(llm_tokenizer)\n",
    "textVocab_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11476859-11b2-45df-964b-b1889bb3a990",
   "metadata": {},
   "source": [
    "## Image Tokens as Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed03d64-3127-4649-be6f-9fff027f69a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of image_token = (1, 32, 32)\n",
    "image_token = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4de271-8ab7-4949-b47d-a09990fed62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of image_token = (32, 32)\n",
    "image_token = image_token.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dfa2d2-6e94-4b7b-a7a9-f789653449e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of image_token = (1024)\n",
    "image_token = image_token.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef5e24-a213-498c-ac76-24a693d3be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac92361-b2e5-4635-a890-a2a0b1b5eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b5263-c164-44f4-b799-1448ec0ea6fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(llm_tokenizer.decode(image_token.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a01cdd-6e57-420e-9021-1295cdf45e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = llm_tokenizer('hi', return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ba736-3eff-44af-94ae-6195b6f47301",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb87b8-a38f-4c98-a46e-ebf15d430957",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = llm_model.generate(\n",
    "    tokens['input_ids'].to(dev),\n",
    "    max_length=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a500793-3448-4a8a-8e0a-39a4d09c4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9932d8b-229d-419b-b209-3c8711f29410",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs1 = llm_tokenizer.convert_ids_to_tokens(outs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c4d72-4b20-4923-a4d6-2679dae5a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_tokenizer.convert_tokens_to_string(outs1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c555e-c4a1-4ea6-828e-4376565c64b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_id_to_string(ids, tokenizer=llm_tokenizer):\n",
    "    out = tokenizer.convert_ids_to_tokens(ids)\n",
    "    out = tokenizer.convert_tokens_to_string(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6384f7e0-95bc-45ca-bbe7-f060c1b3e247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9e68f45-f220-4f68-99b4-2ec4f4e93803",
   "metadata": {},
   "source": [
    "## Changing Image tokens to Text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d24430-98fe-4cab-8d5f-387d00094e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74b816-7d75-4b25-91ec-7307266912af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenMapper(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.mapper = nn.Linear(input_dim, output_dim)\n",
    "        self.mapper.to(device)\n",
    "\n",
    "    def forward(self, one_hot_token):\n",
    "        return self.mapper(one_hot_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39b2ce-583c-4c7f-bbcd-63c6092f4199",
   "metadata": {},
   "source": [
    "## Text LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9b5f8-0a99-4d08-9a29-7ea1456b5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "gpt2_embeddings = gpt2_model.get_input_embeddings().weight\n",
    "\n",
    "gpt2Codebook_len = gpt2_model.config.n_embd\n",
    "gpt2_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8413d-4467-4f4a-b1f4-ff89f690fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2Codebook_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7c86b-911c-4a76-bebf-4ee9e3980e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4824b5b-62a4-4080-8700-83e2e6442db7",
   "metadata": {},
   "source": [
    "## Straight Through Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb5002-2237-426a-9795-972ea2567f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def find_closest_tokens(mapped_vectors, text_token_embeddings):\n",
    "    distances = F.cosine_similarity(mapped_vectors.unsqueeze(1), text_token_embeddings.unsqueeze(0), dim=2)\n",
    "    closest_token_indices = torch.argmax(distances, dim=1)\n",
    "    return closest_token_indices\n",
    "\n",
    "\n",
    "# Constants\n",
    "image_token_dim = imageCodebook_len  # DALL-E image token dimension\n",
    "text_token_dim = gpt2Codebook_len   # Example dimension of text token (like GPT-2)\n",
    "\n",
    "# Create the mapper\n",
    "mapper = TokenMapper(image_token_dim, text_token_dim, device=device)\n",
    "\n",
    "# Example usage\n",
    "def process_image_with_dalle_encoder(image):\n",
    "    z_logits = enc(image)\n",
    "    z = torch.argmax(z_logits, axis=1)\n",
    "    z_ = F.one_hot(z, num_classes=image_token_dim).permute(0, 3, 1, 2).float()\n",
    "    return z_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afdbe0-2789-4c1a-b2b4-792cc84fd05d",
   "metadata": {},
   "source": [
    "## Generate Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca69815-272d-47b6-a2d4-9e8b4a9c6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_indices(mapped_feature_vector):\n",
    "    # mapped_feature_vector has shape (64, 256, 768)\n",
    "    # gpt2_embeddings has shape (50257, 768\n",
    "    # Reshape a to (-1, 768) to treat all vectors in a individually\n",
    "    mapped_fv_reshaped = mapped_feature_vector.view(-1, mapped_feature_vector.shape[-1])\n",
    "\n",
    "    # Compute cosine similarity for each vector in a against all vectors in dict\n",
    "    distances = F.cosine_similarity(mapped_fv_reshaped.unsqueeze(1), gpt2_embeddings.unsqueeze(0), dim=2)\n",
    "\n",
    "    # Find the index of the maximum similarity for each vector\n",
    "    closest_indices = torch.argmax(distances, dim=1)\n",
    "\n",
    "    # Reshape to get back to the original batch and sequence dimension: (64, 256)\n",
    "    closest_indices_reshaped = closest_indices.view(mapped_feature_vector.shape[0], mapped_feature_vector.shape[1])\n",
    "    \n",
    "    return closest_indices_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298cf65-3f3a-41a6-8edb-f3c1cba88ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token_predictions(token_sequences):\n",
    "    # Initialize container for predictions\n",
    "    predictions = torch.zeros(token_sequences.size(), dtype=torch.long)\n",
    "\n",
    "    for i in range(token_sequences.size(1)):  # Iterate over sequence length (256)\n",
    "        # Input tokens up to the current step\n",
    "        input_tokens = token_sequences[:, :i+1]\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt2_model(input_ids=input_tokens)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Get the predicted next token (at the current step)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "        predictions[:, i] = next_token\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf798ea2-42a7-4c26-b4d6-e67ea61cc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt2_ground_truth(mapped_feature_vector):\n",
    "    \n",
    "    closest_indices = find_closest_indices(mapped_feature_vector)\n",
    "    ground_truth = generate_next_token_predictions(closest_indices)\n",
    "    \n",
    "    return ground_truth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec8daa-a7c4-461d-8cfd-391a4d0287d9",
   "metadata": {},
   "source": [
    "## Get Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c1e6e-dbb0-43cf-84d1-a43bbc6e976d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139ed24-d020-461d-992e-c029aea50089",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to a fixed size; adjust as needed\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize (mean, std) for each color channel\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a29062-3951-4668-b8a8-5f088e84a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'path/to/lsun' with the actual path to your LSUN dataset\n",
    "dataset_path = './data'\n",
    "\n",
    "lsun_dataset = datasets.LSUN(root=dataset_path, classes=['classroom_train'], transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c28ed-748a-4109-95e8-d1391e6523c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Adjust based on your memory availability and requirements\n",
    "lsun_loader = DataLoader(lsun_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ae601-c58d-47d0-914f-4840124d9129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244bb1b9-75a5-4c18-953b-1b3434bca223",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce44e2de-350a-4ca7-a437-695573493cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(mapper.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# def train_on_lsun(dataloader, epochs=2):\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, _) in enumerate(lsun_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Process each image through DALL-E encoder to get image tokens\n",
    "        one_hot_image_tokens = process_image_with_dalle_encoder(images.to(device))\n",
    "\n",
    "        # Flatten the tokens for processing with the mapper\n",
    "        flattened_tokens = one_hot_image_tokens.reshape(one_hot_image_tokens.size(0), -1, image_token_dim)\n",
    "\n",
    "        # Initialize container for ground truth tokens\n",
    "        ground_truth_tokens = torch.tensor([], dtype=torch.long, device=device)\n",
    "\n",
    "        # Map tokens and get ground truth from GPT-2\n",
    "        mapped_feature_vector = mapper(token)\n",
    "        ground_truth_token = get_gpt2_ground_truth(mapped_feature_vector).to(device)\n",
    "\n",
    "        # Calculate loss (e.g., CrossEntropyLoss)\n",
    "        # Note: Adjust the loss function as per your requirement and data format\n",
    "        loss = F.cross_entropy(mapped_feature_vector, ground_truth_tokens)\n",
    "\n",
    "        # Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:  # Print loss every 10 batches\n",
    "            print(f\"Epoch {epoch+1}, Batch {i+1}, Loss: {loss.item()}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a72a5-9f8a-461e-b56e-4ef2e93ea610",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5810b15-db43-42bb-b440-f5cb99ae951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263476be-8c91-4862-8d44-852bbb5e0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_feature_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75282530-4262-429e-a393-02c0d135a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2169ae2-e751-418a-a94c-75ac48c6a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d081a-0c5a-4d22-ab58-c643ee59fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = F.cosine_similarity(mapped_feature_vector.unsqueeze(1).cpu(), gpt2_embeddings.unsqueeze(0).cpu(), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5761c9f-b842-460c-a0fe-15b83e672411",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.argmax(temp, dim=1)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5286ce6-640d-49ac-9d03-47e93255e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00248b5e-23fd-4622-8b98-81e477a68467",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992288ea-64fa-44f0-ad2f-b5f262fdd8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(images[4].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b28fb-d3cc-4abb-80c7-6e634d94b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_lsun(lsun_loader, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6548b1-a2e5-4273-acdf-5bfcca9c99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear):  # For fully connected layers\n",
    "        torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc36a98-50db-46ab-ae06-48e5e3d6ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image2Text(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, image_encoder, shift, llm, llm_tokenizer, imageToken_size, imageVocab_size=8192, textVocab_size=50257, device=\"cpu\"):\n",
    "        super(Image2Text, self).__init__()\n",
    "\n",
    "        self.image_encoder = image_encoder.to(device)\n",
    "        self.shift = shift.to(device)\n",
    "        self.norm = nn.BatchNorm1d(imageVocab_size).to(device)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.llm = llm.to(device)\n",
    "        self.llm_tokenizer = llm_tokenizer\n",
    "        self.imageVocab_size = imageVocab_size\n",
    "        self.textVocab_size = textVocab_size\n",
    "        self.device = device\n",
    "        \n",
    "        for params in self.image_encoder.parameters():\n",
    "            params.requires_grad = False\n",
    "            \n",
    "        for params in self.llm.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "        self.shift.apply(init_weights)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.image_encoder(x)  \n",
    "        x = x.permute(0,2,3,1)\n",
    "        x = x.reshape(x.shape[0], -1, x.shape[-1])\n",
    "        \n",
    "        x = x.permute(0, 2, 1)  # Change dims to (N, F, L)\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(0, 2, 1)  # Change dims back to (N, L, F)\n",
    "        \n",
    "        x = self.shift(x)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def getLabel(self, x):\n",
    "        logits = self.image_encoder(x)\n",
    "        img_tokens = torch.argmax(logits, axis=1)\n",
    "        img_tokens = img_tokens.reshape(img_tokens.shape[0], -1)\n",
    "\n",
    "        gpt2_out = self.llm(img_tokens)\n",
    "        gpt2_logits = gpt2_out.logits\n",
    "        gpt2_ids = torch.argmax(gpt2_logits, axis=2)\n",
    "\n",
    "        return gpt2_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b9bfd-685f-4e17-b9aa-427ce912f992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shift = Shift(imageCodebook_len, textVocab_len)\n",
    "model = Image2Text(enc, shift, llm_model, llm_tokenizer, imageToken_size, imageCodebook_len, textVocab_len, device=device)\n",
    "num_epochs = 2  # or whatever number you choose\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-7)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, _) in enumerate(trainloader):  # Assuming you've named your DataLoader 'dataloader'\n",
    "        # Move data to GPU if available\n",
    "        inputs = inputs.to(device)\n",
    "        labels = model.getLabel(inputs)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        with torch.autograd.detect_anomaly():\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "           \n",
    "\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.requires_grad:\n",
    "        #         print(f\"{name} gradients:\")\n",
    "        #         print(param.grad)\n",
    "\n",
    "        # input()\n",
    "\n",
    "        # Print loss every epoch (optional)\n",
    "        if i%5==0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs} Step:{i}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # input()\n",
    "        \n",
    "    scheduler.step()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ce3e8-36d0-4dd4-8f7d-d71e78141552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e0781-d957-4725-a96d-04efd176d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(inputs.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d741dc2-caf4-4331-9858-aa8c6b8ed2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cf90ee-0602-407a-9312-8a47434dcca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2d4b0-03ef-4114-bfbd-7c6d6195725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.image_encoder(inputs.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f2eb4-ef20-4229-a901-e8ce4dcff7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0f53b-efa5-407a-858a-2734b9fb58aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.permute(0,2,3,1)\n",
    "x = x.reshape(x.shape[0], -1, x.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f17ad8e-5bc1-4ca9-bb56-3eff4addc094",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da9d73d-c19c-499d-8203-1ecbc4c68ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = model.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a7261-0fc9-4d72-8e46-45a08e3a2a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b62af2e-9728-43be-8619-201ab0e927f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca59ac-727e-44fd-9b0e-5158378375ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = model.shift.linear1(model.norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f27e2-0bfb-41bc-adb0-751c4457a491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b422ac4-4029-430f-b9f4-eabfad233c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "     for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"{name} gradients:\")\n",
    "                print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ec58b-92de-4405-9266-a38a6be6d062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3de82cf-b4f6-413e-a68d-662286587b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96be6c6-323b-430c-b86f-c43d999c7df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4315d-84b6-47b7-9409-9427f0125aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d045fd-d1e8-4b10-a37b-4d07bb382dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Metrics\n",
    "test_loss = 0.0\n",
    "matrix_loss = []\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, _) in enumerate(testloader):\n",
    "        # Move data to the same device as the model\n",
    "        inputs = inputs.to(device)\n",
    "        labels = model.getLabel(inputs)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        matrix_loss.append(loss.item())\n",
    "        \n",
    "        # if i%50==0:\n",
    "        #     print(f\"Step:{i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# Compute average test loss and accuracy\n",
    "avg_test_loss = test_loss / len(testloader)\n",
    "print(f'average test loss: {avg_test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c4a9c-7881-4db3-987b-c5ef243d63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "window_size = 5\n",
    "smoothed_values = moving_average(matrix_loss, window_size)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(matrix_loss, label=\"Original Loss\")\n",
    "plt.plot(np.arange(window_size-1, len(matrix_loss)), smoothed_values, label=f\"Smoothed Loss (window size={window_size})\", linewidth=2)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dab5c0-7e4e-460a-b34a-91cd1a7e2c40",
   "metadata": {},
   "source": [
    "## What does the image say"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf07273-b28d-40cf-a186-e437376f9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "image, label = next(dataiter)\n",
    "\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b6b61-250b-40e3-9b64-f130bf7664a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(image.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81f184-cc74-4eae-915b-002319b8e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b1b6c-156b-4ac7-9a9b-94079c4ba10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tokens = torch.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d271a2f-8437-42f0-8912-6ef37616419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(img_tokens.shape[0]):\n",
    "    print(convert_id_to_string(img_tokens[i], tokenizer=llm_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d7a7e-b4f8-4f08-94b9-7c38904769c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqvae",
   "language": "python",
   "name": "vqvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
