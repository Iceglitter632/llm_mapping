{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366e69dd-6839-42c1-92fb-8a9766e45b19",
   "metadata": {},
   "source": [
    "## Import Needed Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac1d2f0-21d1-4c06-b4ea-8b5708205a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7be9b7c-2991-4007-8890-2ede65e43c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display, display_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1864141c-f356-49ec-a439-a855160f7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d8057b-5c43-4d55-9641-caf91f0de4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0ac02ed0d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6786ef1-e246-498e-80b7-251c25259685",
   "metadata": {},
   "source": [
    "## Dalle as Image Encoder\n",
    "#### Download VQVAE from DALLE\n",
    "| testing usage\n",
    "```python\n",
    "enc = encoder\n",
    "dec = decoder\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dabb7016-ac33-4ba2-8a49-69bf3ccad26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dall_e import map_pixels, unmap_pixels, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b5f24-3259-4577-ab94-14a1ef133439",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_size = 256\n",
    "\n",
    "def download_image(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return Image.open(io.BytesIO(resp.content))\n",
    "\n",
    "def preprocess(img):\n",
    "    s = min(img.size)\n",
    "    \n",
    "    if s < target_image_size:\n",
    "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "        \n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "    return map_pixels(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf70f0-04fc-4cb3-a77f-080bf1153187",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = load_model(\"https://cdn.openai.com/dall-e/encoder.pkl\", device)\n",
    "# dec = load_model(\"https://cdn.openai.com/dall-e/decoder.pkl\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc9970-5f05-4122-9573-05b2d9e73b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = preprocess(download_image('https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iKIWgaiJUtss/v2/1000x-1.jpg'))\n",
    "display_markdown('Original image:')\n",
    "display(T.ToPILImage(mode='RGB')(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f1f3a-fe2f-4a7b-be56-21b5b4f6bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vocab_len = enc.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfeeb78-2161-4b1e-8b06-7b495ef29cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageVocab_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39b2ce-583c-4c7f-bbcd-63c6092f4199",
   "metadata": {},
   "source": [
    "## Text LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b9076-f123-4ad3-9d3b-a4fe1f6af9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# llm = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model = LlamaForCausalLM.from_pretrained(llm)\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4a9b5f8-0a99-4d08-9a29-7ea1456b5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "llm = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(llm)\n",
    "llm_tokenizer = GPT2Tokenizer.from_pretrained(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f205e7cb-7b7d-4280-bc7e-4f97f3de6048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = model.lm_head.weight\n",
    "# embedding_matrix = model.transformer.wte.weight\n",
    "llm_feature_dim = model.config.hidden_size\n",
    "llm_vocab_len = model.config.vocab_size\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f469ea76-4126-463c-84b4-60ae60a640e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = embeddings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bae8413d-4467-4f4a-b1f4-ff89f690fd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 feature dim length: 768\n",
      "gpt2 vocabulary length: 50257\n",
      "gpt2 embedding shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"gpt2 feature dim length:\", llm_feature_dim)\n",
    "print(\"gpt2 vocabulary length:\", llm_vocab_len)\n",
    "print(\"gpt2 embedding shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e68f45-f220-4f68-99b4-2ec4f4e93803",
   "metadata": {},
   "source": [
    "## Mapper Network\n",
    "\n",
    "map some modality to text token's feature dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36d24430-98fe-4cab-8d5f-387d00094e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e74b816-7d75-4b25-91ec-7307266912af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenMapper(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.mapper = nn.Linear(input_dim, output_dim)\n",
    "        self.mapper.to(device)\n",
    "\n",
    "    def forward(self, one_hot_token):\n",
    "        return self.mapper(one_hot_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b93ca007-4c83-4196-b5a1-ad7bf2c2a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapper\n",
    "# mapper maps vocabulary_size of target modality to feature_dimension size of llm\n",
    "# mapper = TokenMapper(midi_vocab_len, llm_feature_dim, device=device)\n",
    "mapper = TokenMapper(image_vocab_len, llm_feature_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fb20d95-9c20-4d00-92c1-531ab718190c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenMapper(\n",
       "  (mapper): Linear(in_features=16384, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afdbe0-2789-4c1a-b2b4-792cc84fd05d",
   "metadata": {},
   "source": [
    "## Generate Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6298cf65-3f3a-41a6-8edb-f3c1cba88ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token_predictions(token_sequences):\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=token_sequences, output_hidden_states=True)\n",
    "    \n",
    "    return outputs.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c37290d-4b1c-4c74-8f28-378382b80b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(batch_feature_vectors, embeddings):\n",
    "    batch_size, seq_len, embedding_dim = batch_feature_vectors.shape\n",
    "    closest_tokens = torch.zeros((batch_size, seq_len), dtype=torch.long)\n",
    "\n",
    "    # Normalize the embedding matrix\n",
    "    embedding_matrix_norm = F.normalize(embeddings, dim=1)\n",
    "\n",
    "    closest_tokens = torch.zeros((batch_size, seq_len), dtype=torch.long).to(device)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Normalize the feature vectors for the i-th sample in the batch\n",
    "        feature_vectors_norm = F.normalize(batch_feature_vectors[i], dim=1)\n",
    "\n",
    "        # Compute cosine similarity for the entire sequence at once\n",
    "        cosine_similarities = torch.matmul(feature_vectors_norm, embedding_matrix_norm.T)\n",
    "\n",
    "        # Find the token with the highest similarity for each feature vector\n",
    "        closest_tokens[i] = torch.argmax(cosine_similarities, dim=1)\n",
    "\n",
    "    return closest_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec8daa-a7c4-461d-8cfd-391a4d0287d9",
   "metadata": {},
   "source": [
    "## Get Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "095c1e6e-dbb0-43cf-84d1-a43bbc6e976d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139ed24-d020-461d-992e-c029aea50089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((128, 128)),  # Resize to a fixed size; adjust as needed\n",
    "#     transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize (mean, std) for each color channel\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8900e842-7e02-44bc-aafd-7f989c546f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "\n",
    "def resize_and_crop(img):\n",
    "    # Resize while maintaining aspect ratio and center crop\n",
    "    s = min(img.size)\n",
    "    r = image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [image_size])\n",
    "    return img\n",
    "\n",
    "def modified_map_pixels(img):\n",
    "    # Add a batch dimension, apply map_pixels, and then remove the batch dimension\n",
    "    img = img.unsqueeze(0)\n",
    "    img = map_pixels(img)\n",
    "    return img.squeeze(0)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.Lambda(resize_and_crop),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(modified_map_pixels)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99a29062-3951-4668-b8a8-5f088e84a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'path/to/lsun' with the actual path to your LSUN dataset\n",
    "dataset_path = '../data/lsun'\n",
    "\n",
    "lsun_dataset = datasets.LSUN(root=dataset_path, classes=['classroom_train'], transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d52c28ed-748a-4109-95e8-d1391e6523c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 33621\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5  # Adjust based on your memory availability and requirements\n",
    "lsun_loader = DataLoader(lsun_dataset, batch_size=batch_size, shuffle=TrueS)\n",
    "print('dataset size:',len(lsun_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e0c78-6833-4ce3-b8dc-0dc14893645d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "384608d8-dfd0-4f8e-a06d-2ea7309a3f2c",
   "metadata": {},
   "source": [
    "## REINFORCE Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d16ec45-f652-48da-8132-0c5aff6ab98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reinforce_Loss(logits, translated, loss, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Calculate the REINFORCE loss for sequence prediction.\n",
    "\n",
    "    :param logits: Logits from the model, shape [batch_size, seq_len, vocab_size].\n",
    "    :param targets: Ground truth sequence, shape [batch_size, seq_len].\n",
    "    :param rewards: Reward for each step in the sequence, shape [batch_size, seq_len].\n",
    "    :param gamma: Discount factor for future rewards.\n",
    "    :return: The REINFORCE loss (to be maximized).\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = logits.shape\n",
    "\n",
    "    # shape = [batch_size, seq_len, llm_vocab_len]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    log_probs_targets = log_probs.gather(2, translated.unsqueeze(2)).squeeze(2)\n",
    "    \n",
    "    # Create a discount matrix\n",
    "    discount_matrix = torch.zeros((seq_len, seq_len)).to(device)\n",
    "\n",
    "    # Fill the matrix according to the given pattern\n",
    "    for i in range(seq_len):\n",
    "        for j in range(i, seq_len):\n",
    "            discount_matrix[i, j] = gamma ** (j - i)\n",
    "\n",
    "    normalize_factor = discount_matrix.sum(dim=1)\n",
    "    \n",
    "    # Calculate discounted rewards\n",
    "    discounted_loss = loss.unsqueeze(1) * discount_matrix\n",
    "    cumulative_loss = discounted_loss.sum(dim=-1) / normalize_factor\n",
    "    \n",
    "    # Calculate loss\n",
    "    total_loss = torch.sum(log_probs_targets * cumulative_loss.detach()) / batch_size / seq_len\n",
    "    # total_loss = -torch.sum(log_probs_targets * cumulative_loss) / batch_size / seq_len\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244bb1b9-75a5-4c18-953b-1b3434bca223",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "870b3605-366c-4271-817d-b69ab712e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "learning_rate = 1e-5\n",
    "epochs = 1\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0ecacfe-73b7-4d80-b59c-7cbcb01d60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"base_test\"\n",
    "exp_type = \"image\"\n",
    "name = \"vqgan\"\n",
    "experiment_name = f\"{exp_type}/{experiment}/{name}/model={llm}_lr={learning_rate}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b859448-9984-477d-a96c-a2b424627a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Create a SummaryWriter instance (logs will be saved in 'runs' folder)\n",
    "writer = SummaryWriter(log_dir = f'../runs/{experiment_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e98894df-598d-4ad2-88e0-8c84af4904bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(mapper.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rl_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2fb2e-0a55-4525-8c72-51543e1724d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 10.12153434753418\n",
      "Epoch 1, Batch 50, Loss: 9.744757652282715\n",
      "Epoch 1, Batch 100, Loss: 9.322430610656738\n",
      "Epoch 1, Batch 150, Loss: 8.923301696777344\n",
      "Epoch 1, Batch 200, Loss: 8.509306907653809\n",
      "Epoch 1, Batch 250, Loss: 8.168830871582031\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    mapper.train()\n",
    "    # mapper.eval()\n",
    "    for i, (images, _) in enumerate(lsun_loader):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # for dalle\n",
    "        image_token_logits = enc(images.to(device))\n",
    "        ground_truth_tokens = torch.argmax(image_token_logits, dim=1)\n",
    "        one_hot_image_tokens = F.one_hot(ground_truth_tokens, num_classes=imageVocab_len).permute(0, 3, 1, 2).float()\n",
    "\n",
    "        # Logits are to be compared with the next ground truth tokens\n",
    "        ground_truth_tokens = ground_truth_tokens[:,1:]\n",
    "\n",
    "        # Map tokens and get ground truth from LLM\n",
    "        mapped_feature_vector = mapper(one_hot_tokens)\n",
    "\n",
    "        translated_text_tokens = translate(mapped_feature_vector, embeddings)\n",
    "        # Calculate Representation of Last Layer in LLM\n",
    "        final_layer_fv = generate_next_token_predictions(translated_text_tokens)\n",
    "\n",
    "        # Calculate Logits with mapper function\n",
    "        logits = torch.matmul(final_layer_fv, mapper.mapper.weight)\n",
    "        logits = logits[:,:-1]\n",
    "        logits_ = logits.reshape(-1, image_vocab_len)\n",
    "        ground_truth_tokens = ground_truth_tokens.reshape(-1)\n",
    "        \n",
    "        loss = criterion(logits_, ground_truth_tokens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if 'base' in experiment:\n",
    "            writer.add_scalar(\"training/cross_entropy\", loss.item(), epoch*len(lsun_loader)+i)\n",
    "        # RL Loss\n",
    "        if 'rl' in experiment:\n",
    "            \n",
    "            action_logits = torch.matmul(mapped_feature_vector, embeddings.T)\n",
    "            with torch.no_grad():\n",
    "                ce_loss = rl_criterion(logits_, ground_truth_tokens)\n",
    "            ground_truth_tokens = ground_truth_tokens.reshape(batch_size, -1)\n",
    "            ce_loss = ce_loss.reshape(batch_size, -1)\n",
    "    \n",
    "            loss = Reinforce_Loss(action_logits, translated_text_tokens, ce_loss)\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log the losses\n",
    "            writer.add_scalars(\n",
    "                \"training\",\n",
    "                {\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"cross_entropy\": ce_loss.mean().item(),\n",
    "                },\n",
    "                epoch * len(lsun_loader) + i\n",
    "            )\n",
    "            \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {i}, Loss: {loss.item()}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed.\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0c8bd-bd0f-4d58-a50a-98a0ac8b6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f\"../models/{experiment_name}\").mkdir(parents=True, exist_ok=True)\n",
    "torch.save(mapper.state_dict(), f\"../models/{experiment_name}/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be09272-d867-42ae-bcaf-672af7863786",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqvae",
   "language": "python",
   "name": "vqvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
