{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366e69dd-6839-42c1-92fb-8a9766e45b19",
   "metadata": {},
   "source": [
    "## Import Needed Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac1d2f0-21d1-4c06-b4ea-8b5708205a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os, sys\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6786ef1-e246-498e-80b7-251c25259685",
   "metadata": {},
   "source": [
    "## Dalle as Image Encoder\n",
    "#### Download VQVAE from DALLE\n",
    "| testing usage\n",
    "```python\n",
    "enc = encoder\n",
    "dec = decoder\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be9b7c-2991-4007-8890-2ede65e43c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from dall_e import map_pixels, unmap_pixels, load_model\n",
    "from IPython.display import display, display_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864141c-f356-49ec-a439-a855160f7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b5f24-3259-4577-ab94-14a1ef133439",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_size = 256\n",
    "\n",
    "def download_image(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return Image.open(io.BytesIO(resp.content))\n",
    "\n",
    "def preprocess(img):\n",
    "    s = min(img.size)\n",
    "    \n",
    "    if s < target_image_size:\n",
    "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "        \n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "    return map_pixels(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf70f0-04fc-4cb3-a77f-080bf1153187",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = load_model(\"https://cdn.openai.com/dall-e/encoder.pkl\", device)\n",
    "# dec = load_model(\"https://cdn.openai.com/dall-e/decoder.pkl\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc9970-5f05-4122-9573-05b2d9e73b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = preprocess(download_image('https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iKIWgaiJUtss/v2/1000x-1.jpg'))\n",
    "display_markdown('Original image:')\n",
    "display(T.ToPILImage(mode='RGB')(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f1f3a-fe2f-4a7b-be56-21b5b4f6bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageVocab_len = enc.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfeeb78-2161-4b1e-8b06-7b495ef29cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageCodebook_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab575dd9-c670-4b18-89dd-4f32a71e288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_with_encoder(image):\n",
    "    z = enc(image)\n",
    "    z = torch.argmax(z, axis=1)\n",
    "    z_ = F.one_hot(z, num_classes=imageCodebook_len).permute(0, 3, 1, 2).float()\n",
    "    return z_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f992d-a311-4d18-b077-89fd103ebcce",
   "metadata": {},
   "source": [
    "## VQGAN as Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0559534-03de-4029-aef6-a994f39a3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vqgan_jax.modeling_flax_vqgan import VQModel\n",
    "# from transformers import VQGanForPreTraining\n",
    "# from transformers import VQGanProcessor\n",
    "\n",
    "# Load the pre-trained VQGAN model and its processor\n",
    "# checkpoint = \"dalle-mini/vqgan_imagenet_f16_16384\"\n",
    "# model = VQModel.from_pretrained(checkpoint)\n",
    "# processor = VQGanProcessor.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e2b06-9f22-4853-8055-5879bed2e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_image(url):\n",
    "#     resp = requests.get(url)\n",
    "#     resp.raise_for_status()\n",
    "#     return Image.open(io.BytesIO(resp.content))\n",
    "\n",
    "# def preprocess_vqgan(x):\n",
    "#   x = 2.*x - 1.\n",
    "#   return x\n",
    "\n",
    "# def custom_to_pil(x):\n",
    "#   x = np.clip(x, -1., 1.)\n",
    "#   x = (x + 1.)/2.\n",
    "#   x = (255*x).astype(np.uint8)\n",
    "#   x = Image.fromarray(x)\n",
    "#   if not x.mode == \"RGB\":\n",
    "#     x = x.convert(\"RGB\")\n",
    "#   return x\n",
    "\n",
    "# def preprocess(img, target_image_size=256,):\n",
    "#     s = min(img.size)\n",
    "    \n",
    "#     if s < target_image_size:\n",
    "#         raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "        \n",
    "#     r = target_image_size / s\n",
    "#     s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "#     img = TF.resize(img, s, interpolation=Image.LANCZOS)\n",
    "#     img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "#     img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "#     return img.permute(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ce53b-98e7-46cf-9ffa-ab87837d1350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from torchvision.transforms import InterpolationMode\n",
    "# def resize_image(image, size=256):\n",
    "#     s = min(image.size)\n",
    "#     r = size / s\n",
    "#     s = (round(r * image.size[1]), round(r * image.size[0]))\n",
    "#     image = TF.resize(image, s, interpolation=InterpolationMode.LANCZOS)\n",
    "#     image = TF.center_crop(image, output_size = 2 * [size])\n",
    "#     image = np.expand_dims(np.array(image), axis=0)\n",
    "#     return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8021919-8047-4232-bc01-6a971b315a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='https://heibox.uni-heidelberg.de/f/7bb608381aae4539ba7a/?dl=1'\n",
    "# size=256\n",
    "# image = download_image(url)\n",
    "# image = resize_image(image)\n",
    "# image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8792470-5521-4f88-bf30-b9d4615b879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(T.ToPILImage(mode='RGB')(image[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7cb35-e3d1-4709-bf60-bfcf106aee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, id = model.encode(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab439a5-04bd-4801-8a5a-503ed38ce773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = model.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb552fe-9f67-4216-b4db-c705b57c2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCodebook_len = 16384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca293fa4-be2e-4d0d-9923-3be8f9288f28",
   "metadata": {},
   "source": [
    "## MIDITOK as MIDI Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d6f20a-966b-4ac8-8f77-3e4b1e41e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok import REMIPlus, TokenizerConfig\n",
    "from miditoolkit import MidiFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98bb450-9744-4d85-b907-84c2d2136644",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_PARAMS = {\n",
    "    \"pitch_range\": (21, 109),\n",
    "    \"beat_res\": {(0, 4): 8, (4, 12): 4},\n",
    "    \"num_velocities\": 32,\n",
    "    \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n",
    "    \"use_chords\": True,\n",
    "    \"use_rests\": False,\n",
    "    \"use_tempos\": True,\n",
    "    \"use_time_signatures\": False,\n",
    "    \"use_programs\": False,\n",
    "    \"num_tempos\": 32,  # number of tempo bins\n",
    "    \"tempo_range\": (40, 250),  # (min, max)\n",
    "}\n",
    "config = TokenizerConfig(**TOKENIZER_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f63b64-5a9f-48dc-bdc2-30c7138a1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = REMIPlus(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d95b91a-91e0-4c75-a9b2-5666123b003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi = MidiFile(\"../data/midi/MMD_MIDI/0/0/0/00000ec8a66b6bd2ef809b0443eeae41.mid\")\n",
    "tokens = tokenizer(midi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57218eba-5c04-4a02-a48e-a33a5a450e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 420, 189, 267, 284, 62, 124, 126, 191, 284, 50, 124, 126, 193, 284, 66, 124, 126, 195, 284]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokens.ids\n",
    "print(token_ids[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e336aae-81c9-4c02-b3eb-e9c84253fbf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD_None': 0,\n",
       " 'BOS_None': 1,\n",
       " 'EOS_None': 2,\n",
       " 'MASK_None': 3,\n",
       " 'Bar_None': 4,\n",
       " 'Pitch_21': 5,\n",
       " 'Pitch_22': 6,\n",
       " 'Pitch_23': 7,\n",
       " 'Pitch_24': 8,\n",
       " 'Pitch_25': 9,\n",
       " 'Pitch_26': 10,\n",
       " 'Pitch_27': 11,\n",
       " 'Pitch_28': 12,\n",
       " 'Pitch_29': 13,\n",
       " 'Pitch_30': 14,\n",
       " 'Pitch_31': 15,\n",
       " 'Pitch_32': 16,\n",
       " 'Pitch_33': 17,\n",
       " 'Pitch_34': 18,\n",
       " 'Pitch_35': 19,\n",
       " 'Pitch_36': 20,\n",
       " 'Pitch_37': 21,\n",
       " 'Pitch_38': 22,\n",
       " 'Pitch_39': 23,\n",
       " 'Pitch_40': 24,\n",
       " 'Pitch_41': 25,\n",
       " 'Pitch_42': 26,\n",
       " 'Pitch_43': 27,\n",
       " 'Pitch_44': 28,\n",
       " 'Pitch_45': 29,\n",
       " 'Pitch_46': 30,\n",
       " 'Pitch_47': 31,\n",
       " 'Pitch_48': 32,\n",
       " 'Pitch_49': 33,\n",
       " 'Pitch_50': 34,\n",
       " 'Pitch_51': 35,\n",
       " 'Pitch_52': 36,\n",
       " 'Pitch_53': 37,\n",
       " 'Pitch_54': 38,\n",
       " 'Pitch_55': 39,\n",
       " 'Pitch_56': 40,\n",
       " 'Pitch_57': 41,\n",
       " 'Pitch_58': 42,\n",
       " 'Pitch_59': 43,\n",
       " 'Pitch_60': 44,\n",
       " 'Pitch_61': 45,\n",
       " 'Pitch_62': 46,\n",
       " 'Pitch_63': 47,\n",
       " 'Pitch_64': 48,\n",
       " 'Pitch_65': 49,\n",
       " 'Pitch_66': 50,\n",
       " 'Pitch_67': 51,\n",
       " 'Pitch_68': 52,\n",
       " 'Pitch_69': 53,\n",
       " 'Pitch_70': 54,\n",
       " 'Pitch_71': 55,\n",
       " 'Pitch_72': 56,\n",
       " 'Pitch_73': 57,\n",
       " 'Pitch_74': 58,\n",
       " 'Pitch_75': 59,\n",
       " 'Pitch_76': 60,\n",
       " 'Pitch_77': 61,\n",
       " 'Pitch_78': 62,\n",
       " 'Pitch_79': 63,\n",
       " 'Pitch_80': 64,\n",
       " 'Pitch_81': 65,\n",
       " 'Pitch_82': 66,\n",
       " 'Pitch_83': 67,\n",
       " 'Pitch_84': 68,\n",
       " 'Pitch_85': 69,\n",
       " 'Pitch_86': 70,\n",
       " 'Pitch_87': 71,\n",
       " 'Pitch_88': 72,\n",
       " 'Pitch_89': 73,\n",
       " 'Pitch_90': 74,\n",
       " 'Pitch_91': 75,\n",
       " 'Pitch_92': 76,\n",
       " 'Pitch_93': 77,\n",
       " 'Pitch_94': 78,\n",
       " 'Pitch_95': 79,\n",
       " 'Pitch_96': 80,\n",
       " 'Pitch_97': 81,\n",
       " 'Pitch_98': 82,\n",
       " 'Pitch_99': 83,\n",
       " 'Pitch_100': 84,\n",
       " 'Pitch_101': 85,\n",
       " 'Pitch_102': 86,\n",
       " 'Pitch_103': 87,\n",
       " 'Pitch_104': 88,\n",
       " 'Pitch_105': 89,\n",
       " 'Pitch_106': 90,\n",
       " 'Pitch_107': 91,\n",
       " 'Pitch_108': 92,\n",
       " 'Velocity_3': 93,\n",
       " 'Velocity_7': 94,\n",
       " 'Velocity_11': 95,\n",
       " 'Velocity_15': 96,\n",
       " 'Velocity_19': 97,\n",
       " 'Velocity_23': 98,\n",
       " 'Velocity_27': 99,\n",
       " 'Velocity_31': 100,\n",
       " 'Velocity_35': 101,\n",
       " 'Velocity_39': 102,\n",
       " 'Velocity_43': 103,\n",
       " 'Velocity_47': 104,\n",
       " 'Velocity_51': 105,\n",
       " 'Velocity_55': 106,\n",
       " 'Velocity_59': 107,\n",
       " 'Velocity_63': 108,\n",
       " 'Velocity_67': 109,\n",
       " 'Velocity_71': 110,\n",
       " 'Velocity_75': 111,\n",
       " 'Velocity_79': 112,\n",
       " 'Velocity_83': 113,\n",
       " 'Velocity_87': 114,\n",
       " 'Velocity_91': 115,\n",
       " 'Velocity_95': 116,\n",
       " 'Velocity_99': 117,\n",
       " 'Velocity_103': 118,\n",
       " 'Velocity_107': 119,\n",
       " 'Velocity_111': 120,\n",
       " 'Velocity_115': 121,\n",
       " 'Velocity_119': 122,\n",
       " 'Velocity_123': 123,\n",
       " 'Velocity_127': 124,\n",
       " 'Duration_0.1.8': 125,\n",
       " 'Duration_0.2.8': 126,\n",
       " 'Duration_0.3.8': 127,\n",
       " 'Duration_0.4.8': 128,\n",
       " 'Duration_0.5.8': 129,\n",
       " 'Duration_0.6.8': 130,\n",
       " 'Duration_0.7.8': 131,\n",
       " 'Duration_1.0.8': 132,\n",
       " 'Duration_1.1.8': 133,\n",
       " 'Duration_1.2.8': 134,\n",
       " 'Duration_1.3.8': 135,\n",
       " 'Duration_1.4.8': 136,\n",
       " 'Duration_1.5.8': 137,\n",
       " 'Duration_1.6.8': 138,\n",
       " 'Duration_1.7.8': 139,\n",
       " 'Duration_2.0.8': 140,\n",
       " 'Duration_2.1.8': 141,\n",
       " 'Duration_2.2.8': 142,\n",
       " 'Duration_2.3.8': 143,\n",
       " 'Duration_2.4.8': 144,\n",
       " 'Duration_2.5.8': 145,\n",
       " 'Duration_2.6.8': 146,\n",
       " 'Duration_2.7.8': 147,\n",
       " 'Duration_3.0.8': 148,\n",
       " 'Duration_3.1.8': 149,\n",
       " 'Duration_3.2.8': 150,\n",
       " 'Duration_3.3.8': 151,\n",
       " 'Duration_3.4.8': 152,\n",
       " 'Duration_3.5.8': 153,\n",
       " 'Duration_3.6.8': 154,\n",
       " 'Duration_3.7.8': 155,\n",
       " 'Duration_4.0.4': 156,\n",
       " 'Duration_4.1.4': 157,\n",
       " 'Duration_4.2.4': 158,\n",
       " 'Duration_4.3.4': 159,\n",
       " 'Duration_5.0.4': 160,\n",
       " 'Duration_5.1.4': 161,\n",
       " 'Duration_5.2.4': 162,\n",
       " 'Duration_5.3.4': 163,\n",
       " 'Duration_6.0.4': 164,\n",
       " 'Duration_6.1.4': 165,\n",
       " 'Duration_6.2.4': 166,\n",
       " 'Duration_6.3.4': 167,\n",
       " 'Duration_7.0.4': 168,\n",
       " 'Duration_7.1.4': 169,\n",
       " 'Duration_7.2.4': 170,\n",
       " 'Duration_7.3.4': 171,\n",
       " 'Duration_8.0.4': 172,\n",
       " 'Duration_8.1.4': 173,\n",
       " 'Duration_8.2.4': 174,\n",
       " 'Duration_8.3.4': 175,\n",
       " 'Duration_9.0.4': 176,\n",
       " 'Duration_9.1.4': 177,\n",
       " 'Duration_9.2.4': 178,\n",
       " 'Duration_9.3.4': 179,\n",
       " 'Duration_10.0.4': 180,\n",
       " 'Duration_10.1.4': 181,\n",
       " 'Duration_10.2.4': 182,\n",
       " 'Duration_10.3.4': 183,\n",
       " 'Duration_11.0.4': 184,\n",
       " 'Duration_11.1.4': 185,\n",
       " 'Duration_11.2.4': 186,\n",
       " 'Duration_11.3.4': 187,\n",
       " 'Duration_12.0.4': 188,\n",
       " 'Position_0': 189,\n",
       " 'Position_1': 190,\n",
       " 'Position_2': 191,\n",
       " 'Position_3': 192,\n",
       " 'Position_4': 193,\n",
       " 'Position_5': 194,\n",
       " 'Position_6': 195,\n",
       " 'Position_7': 196,\n",
       " 'Position_8': 197,\n",
       " 'Position_9': 198,\n",
       " 'Position_10': 199,\n",
       " 'Position_11': 200,\n",
       " 'Position_12': 201,\n",
       " 'Position_13': 202,\n",
       " 'Position_14': 203,\n",
       " 'Position_15': 204,\n",
       " 'Position_16': 205,\n",
       " 'Position_17': 206,\n",
       " 'Position_18': 207,\n",
       " 'Position_19': 208,\n",
       " 'Position_20': 209,\n",
       " 'Position_21': 210,\n",
       " 'Position_22': 211,\n",
       " 'Position_23': 212,\n",
       " 'Position_24': 213,\n",
       " 'Position_25': 214,\n",
       " 'Position_26': 215,\n",
       " 'Position_27': 216,\n",
       " 'Position_28': 217,\n",
       " 'Position_29': 218,\n",
       " 'Position_30': 219,\n",
       " 'Position_31': 220,\n",
       " 'Position_32': 221,\n",
       " 'Position_33': 222,\n",
       " 'Position_34': 223,\n",
       " 'Position_35': 224,\n",
       " 'Position_36': 225,\n",
       " 'Position_37': 226,\n",
       " 'Position_38': 227,\n",
       " 'Position_39': 228,\n",
       " 'Position_40': 229,\n",
       " 'Position_41': 230,\n",
       " 'Position_42': 231,\n",
       " 'Position_43': 232,\n",
       " 'Position_44': 233,\n",
       " 'Position_45': 234,\n",
       " 'Position_46': 235,\n",
       " 'Position_47': 236,\n",
       " 'Chord_min': 237,\n",
       " 'Chord_maj': 238,\n",
       " 'Chord_dim': 239,\n",
       " 'Chord_aug': 240,\n",
       " 'Chord_sus2': 241,\n",
       " 'Chord_sus4': 242,\n",
       " 'Chord_7dom': 243,\n",
       " 'Chord_7min': 244,\n",
       " 'Chord_7maj': 245,\n",
       " 'Chord_7halfdim': 246,\n",
       " 'Chord_7dim': 247,\n",
       " 'Chord_7aug': 248,\n",
       " 'Chord_9maj': 249,\n",
       " 'Chord_9min': 250,\n",
       " 'Tempo_40.0': 251,\n",
       " 'Tempo_46.77': 252,\n",
       " 'Tempo_53.55': 253,\n",
       " 'Tempo_60.32': 254,\n",
       " 'Tempo_67.1': 255,\n",
       " 'Tempo_73.87': 256,\n",
       " 'Tempo_80.65': 257,\n",
       " 'Tempo_87.42': 258,\n",
       " 'Tempo_94.19': 259,\n",
       " 'Tempo_100.97': 260,\n",
       " 'Tempo_107.74': 261,\n",
       " 'Tempo_114.52': 262,\n",
       " 'Tempo_121.29': 263,\n",
       " 'Tempo_128.06': 264,\n",
       " 'Tempo_134.84': 265,\n",
       " 'Tempo_141.61': 266,\n",
       " 'Tempo_148.39': 267,\n",
       " 'Tempo_155.16': 268,\n",
       " 'Tempo_161.94': 269,\n",
       " 'Tempo_168.71': 270,\n",
       " 'Tempo_175.48': 271,\n",
       " 'Tempo_182.26': 272,\n",
       " 'Tempo_189.03': 273,\n",
       " 'Tempo_195.81': 274,\n",
       " 'Tempo_202.58': 275,\n",
       " 'Tempo_209.35': 276,\n",
       " 'Tempo_216.13': 277,\n",
       " 'Tempo_222.9': 278,\n",
       " 'Tempo_229.68': 279,\n",
       " 'Tempo_236.45': 280,\n",
       " 'Tempo_243.23': 281,\n",
       " 'Tempo_250.0': 282,\n",
       " 'Program_-1': 283,\n",
       " 'Program_0': 284,\n",
       " 'Program_1': 285,\n",
       " 'Program_2': 286,\n",
       " 'Program_3': 287,\n",
       " 'Program_4': 288,\n",
       " 'Program_5': 289,\n",
       " 'Program_6': 290,\n",
       " 'Program_7': 291,\n",
       " 'Program_8': 292,\n",
       " 'Program_9': 293,\n",
       " 'Program_10': 294,\n",
       " 'Program_11': 295,\n",
       " 'Program_12': 296,\n",
       " 'Program_13': 297,\n",
       " 'Program_14': 298,\n",
       " 'Program_15': 299,\n",
       " 'Program_16': 300,\n",
       " 'Program_17': 301,\n",
       " 'Program_18': 302,\n",
       " 'Program_19': 303,\n",
       " 'Program_20': 304,\n",
       " 'Program_21': 305,\n",
       " 'Program_22': 306,\n",
       " 'Program_23': 307,\n",
       " 'Program_24': 308,\n",
       " 'Program_25': 309,\n",
       " 'Program_26': 310,\n",
       " 'Program_27': 311,\n",
       " 'Program_28': 312,\n",
       " 'Program_29': 313,\n",
       " 'Program_30': 314,\n",
       " 'Program_31': 315,\n",
       " 'Program_32': 316,\n",
       " 'Program_33': 317,\n",
       " 'Program_34': 318,\n",
       " 'Program_35': 319,\n",
       " 'Program_36': 320,\n",
       " 'Program_37': 321,\n",
       " 'Program_38': 322,\n",
       " 'Program_39': 323,\n",
       " 'Program_40': 324,\n",
       " 'Program_41': 325,\n",
       " 'Program_42': 326,\n",
       " 'Program_43': 327,\n",
       " 'Program_44': 328,\n",
       " 'Program_45': 329,\n",
       " 'Program_46': 330,\n",
       " 'Program_47': 331,\n",
       " 'Program_48': 332,\n",
       " 'Program_49': 333,\n",
       " 'Program_50': 334,\n",
       " 'Program_51': 335,\n",
       " 'Program_52': 336,\n",
       " 'Program_53': 337,\n",
       " 'Program_54': 338,\n",
       " 'Program_55': 339,\n",
       " 'Program_56': 340,\n",
       " 'Program_57': 341,\n",
       " 'Program_58': 342,\n",
       " 'Program_59': 343,\n",
       " 'Program_60': 344,\n",
       " 'Program_61': 345,\n",
       " 'Program_62': 346,\n",
       " 'Program_63': 347,\n",
       " 'Program_64': 348,\n",
       " 'Program_65': 349,\n",
       " 'Program_66': 350,\n",
       " 'Program_67': 351,\n",
       " 'Program_68': 352,\n",
       " 'Program_69': 353,\n",
       " 'Program_70': 354,\n",
       " 'Program_71': 355,\n",
       " 'Program_72': 356,\n",
       " 'Program_73': 357,\n",
       " 'Program_74': 358,\n",
       " 'Program_75': 359,\n",
       " 'Program_76': 360,\n",
       " 'Program_77': 361,\n",
       " 'Program_78': 362,\n",
       " 'Program_79': 363,\n",
       " 'Program_80': 364,\n",
       " 'Program_81': 365,\n",
       " 'Program_82': 366,\n",
       " 'Program_83': 367,\n",
       " 'Program_84': 368,\n",
       " 'Program_85': 369,\n",
       " 'Program_86': 370,\n",
       " 'Program_87': 371,\n",
       " 'Program_88': 372,\n",
       " 'Program_89': 373,\n",
       " 'Program_90': 374,\n",
       " 'Program_91': 375,\n",
       " 'Program_92': 376,\n",
       " 'Program_93': 377,\n",
       " 'Program_94': 378,\n",
       " 'Program_95': 379,\n",
       " 'Program_96': 380,\n",
       " 'Program_97': 381,\n",
       " 'Program_98': 382,\n",
       " 'Program_99': 383,\n",
       " 'Program_100': 384,\n",
       " 'Program_101': 385,\n",
       " 'Program_102': 386,\n",
       " 'Program_103': 387,\n",
       " 'Program_104': 388,\n",
       " 'Program_105': 389,\n",
       " 'Program_106': 390,\n",
       " 'Program_107': 391,\n",
       " 'Program_108': 392,\n",
       " 'Program_109': 393,\n",
       " 'Program_110': 394,\n",
       " 'Program_111': 395,\n",
       " 'Program_112': 396,\n",
       " 'Program_113': 397,\n",
       " 'Program_114': 398,\n",
       " 'Program_115': 399,\n",
       " 'Program_116': 400,\n",
       " 'Program_117': 401,\n",
       " 'Program_118': 402,\n",
       " 'Program_119': 403,\n",
       " 'Program_120': 404,\n",
       " 'Program_121': 405,\n",
       " 'Program_122': 406,\n",
       " 'Program_123': 407,\n",
       " 'Program_124': 408,\n",
       " 'Program_125': 409,\n",
       " 'Program_126': 410,\n",
       " 'Program_127': 411,\n",
       " 'TimeSig_3/8': 412,\n",
       " 'TimeSig_12/8': 413,\n",
       " 'TimeSig_6/8': 414,\n",
       " 'TimeSig_5/4': 415,\n",
       " 'TimeSig_6/4': 416,\n",
       " 'TimeSig_3/4': 417,\n",
       " 'TimeSig_2/4': 418,\n",
       " 'TimeSig_1/4': 419,\n",
       " 'TimeSig_4/4': 420}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midiVocab_len = len(tokenizer.vocab)\n",
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf7d244-0201-4d70-b14e-8868d1993f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47f0c3-6a4b-4c1b-a4a2-942a2881b0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc20fe7-a4b4-4166-b097-307db6244984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2fe8c5-1822-41e8-a9ba-7dc399a4d68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d8f836-88f0-44aa-ba82-d605c8429d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c42a1-a42b-40a4-b8fc-c79145f5fe8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc57ad-c724-4d52-8c6b-ffccf0931839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489e4b8-d218-41a3-9f99-b6ace9c43b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a39b2ce-583c-4c7f-bbcd-63c6092f4199",
   "metadata": {},
   "source": [
    "## Text LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b9076-f123-4ad3-9d3b-a4fe1f6af9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# llm = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model = LlamaForCausalLM.from_pretrained(llm)\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9b5f8-0a99-4d08-9a29-7ea1456b5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "llm = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(llm)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d201283-22de-4789-9f3b-36158d615189",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27636b-6a0c-4c8b-97ea-9901dcad559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38523524-744d-4f26-979d-bda16cf15e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# embed_tokens = nn.Embedding(model.config.vocab_size, model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620babb7-7b61-46a4-a6cd-b7ed31c166bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205e7cb-7b7d-4280-bc7e-4f97f3de6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.lm_head.weight\n",
    "# embedding_matrix = model.transformer.wte.weight\n",
    "codebook_len = model.config.hidden_size\n",
    "vocab_len = model.config.vocab_size\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9cf717-a90e-47da-96e5-2c7bec6262d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469ea76-4126-463c-84b4-60ae60a640e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = embeddings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8413d-4467-4f4a-b1f4-ff89f690fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gpt2 codebook length:\", codebook_len)\n",
    "print(\"gpt2 vocabulary length:\", vocab_len)\n",
    "print(\"gpt2 embedding shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfcd152-c0b7-4ee2-b23f-71ea3ad7a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_llm_with_embeddings(embeddings):\n",
    "    \"\"\"\n",
    "    Forward pass through GPT-2 for sequential token prediction logits from embeddings.\n",
    "\n",
    "    :param embeddings: Embeddings of the sequence, shape [batch_size, seq_len, embedding_dim].\n",
    "    :return: Tensor of logits for token predictions, shape [batch_size, seq_len, vocab_size].\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = embeddings.size()\n",
    "    vocab_size = model.config.vocab_size\n",
    "    predicted_logits = torch.zeros((batch_size, seq_len, vocab_size), device=embeddings.device)\n",
    "    \n",
    "    gpt2_model.eval()\n",
    "\n",
    "    embeddings = embeddings.detach()\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        # Use embeddings up to the i-th position to predict the next token\n",
    "        input_embeddings = embeddings[:, :i+1, :]\n",
    "\n",
    "        # Forward pass through GPT-2\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt2_model(inputs_embeds=input_embeddings)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Get the logits for the next position (i+1)\n",
    "        predicted_logits[:, i, :] = logits[:, -1, :]  # Last token in the sequence\n",
    "\n",
    "    predicted_logits.requires_grad = True\n",
    "\n",
    "    return predicted_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e68f45-f220-4f68-99b4-2ec4f4e93803",
   "metadata": {},
   "source": [
    "## Changing Image tokens to Text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d24430-98fe-4cab-8d5f-387d00094e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74b816-7d75-4b25-91ec-7307266912af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenMapper(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.mapper = nn.Linear(input_dim, output_dim)\n",
    "        self.mapper.to(device)\n",
    "\n",
    "    def forward(self, one_hot_token):\n",
    "        return self.mapper(one_hot_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ca007-4c83-4196-b5a1-ad7bf2c2a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapper\n",
    "# mapper maps a 8192 to a 768\n",
    "mapper = TokenMapper(imageCodebook_len, codebook_len, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb20d95-9c20-4d00-92c1-531ab718190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afdbe0-2789-4c1a-b2b4-792cc84fd05d",
   "metadata": {},
   "source": [
    "## Generate Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd9922-177f-46ca-8c14-c741ec645031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_indices_cosine(mapped_feature_vector, batch_size=10):\n",
    "    # mapped_fv_reshaped has shape (-1, 768)\n",
    "    mapped_fv_reshaped = mapped_feature_vector.view(-1, mapped_feature_vector.shape[-1])\n",
    "    \n",
    "    closest_indices = []\n",
    "    for i in range(0, mapped_fv_reshaped.size(0), batch_size):\n",
    "        # Process in smaller batches\n",
    "        batch_fv = mapped_fv_reshaped[i:i+batch_size]\n",
    "\n",
    "        # Compute cosine similarity for the batch\n",
    "        distances_batch = F.cosine_similarity(batch_fv.unsqueeze(1), gpt2_embeddings.unsqueeze(0), dim=2)\n",
    "\n",
    "        # Find the index of the maximum similarity for each vector in the batch\n",
    "        closest_indices_batch = torch.argmax(distances_batch, dim=1)\n",
    "        closest_indices.append(closest_indices_batch)\n",
    "\n",
    "    # Concatenate results from all batches\n",
    "    closest_indices = torch.cat(closest_indices, dim=0)\n",
    "\n",
    "    # Reshape to the original batch and sequence dimension\n",
    "    closest_indices_reshaped = closest_indices.view(mapped_feature_vector.shape[0], mapped_feature_vector.shape[1]).to(device)\n",
    "    \n",
    "    return closest_indices_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca69815-272d-47b6-a2d4-9e8b4a9c6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_gpt2_token(batch_feature_vectors):\n",
    "    \"\"\"\n",
    "    Find the GPT-2 token whose embedding is closest to the given feature vector.\n",
    "\n",
    "    :param feature_vector: The feature vector (from the mapper). Shape: (embedding_dim,)\n",
    "    :param embedding_matrix: GPT-2's embedding matrix. Shape: (vocab_size, embedding_dim)\n",
    "    :return: The ID of the closest token.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, embedding_dim = batch_feature_vectors.shape\n",
    "    closest_tokens = torch.zeros((batch_size, seq_len), dtype=torch.long)\n",
    "    \n",
    "    # Normalize the feature vector and the embedding matrix for cosine similarity\n",
    "    embedding_matrix_norm = F.normalize(gpt2_embeddings, dim=1)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            # Normalize the feature vector\n",
    "            feature_vector_norm = F.normalize(batch_feature_vectors[i, j].unsqueeze(0), dim=1)\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            cosine_similarities = torch.matmul(feature_vector_norm, embedding_matrix_norm.T).squeeze(0)\n",
    "\n",
    "            # Find the token with the highest similarity\n",
    "            closest_token_id = torch.argmax(cosine_similarities).item()\n",
    "            closest_tokens[i, j] = closest_token_id\n",
    "\n",
    "    return closest_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298cf65-3f3a-41a6-8edb-f3c1cba88ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token_predictions(token_sequences):\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=token_sequences, output_hidden_states=True)\n",
    "    \n",
    "    return outputs.hidden_states[-1]\n",
    "        \n",
    "    # return predictions\n",
    "    # return logits, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad186c-171a-419c-9d6a-3f2a39e4cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_token_logits(batch_feature_vectors, embeddings):\n",
    "        \"\"\"\n",
    "        Find the LLM token who has the highest dot product to the given feature vector\n",
    "        This acts as the action of our REINFORCE algorithm\n",
    "        \n",
    "        return prob: Probability of each token getting chosen => shape:(batch_size, seq_len, llm_vocabulary_size)\n",
    "        return closest_tokens: The token with the highest probability => shape:(batch_size, seq_len) \n",
    "        \"\"\"\n",
    "        dot_product = torch.matmul(batch_feature_vectors, embeddings.T)\n",
    "        probs = F.softmax(dot_product, dim=-1)\n",
    "        closest_tokens = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        return closest_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf798ea2-42a7-4c26-b4d6-e67ea61cc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth(mapped_feature_vector, embeddings):\n",
    "    \n",
    "    ground_truth = find_closest_token_logits(mapped_feature_vector, embeddings)\n",
    "\n",
    "    return ground_truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37290d-4b1c-4c74-8f28-378382b80b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(batch_feature_vectors, embeddings):\n",
    "    batch_size, seq_len, embedding_dim = batch_feature_vectors.shape\n",
    "    closest_tokens = torch.zeros((batch_size, seq_len), dtype=torch.long)\n",
    "\n",
    "    # Normalize the embedding matrix\n",
    "    embedding_matrix_norm = F.normalize(embeddings, dim=1)\n",
    "\n",
    "    closest_tokens = torch.zeros((batch_size, seq_len), dtype=torch.long).to(device)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Normalize the feature vectors for the i-th sample in the batch\n",
    "        feature_vectors_norm = F.normalize(batch_feature_vectors[i], dim=1)\n",
    "\n",
    "        # Compute cosine similarity for the entire sequence at once\n",
    "        cosine_similarities = torch.matmul(feature_vectors_norm, embedding_matrix_norm.T)\n",
    "\n",
    "        # Find the token with the highest similarity for each feature vector\n",
    "        closest_tokens[i] = torch.argmax(cosine_similarities, dim=1)\n",
    "\n",
    "    return closest_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec8daa-a7c4-461d-8cfd-391a4d0287d9",
   "metadata": {},
   "source": [
    "## Get Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c1e6e-dbb0-43cf-84d1-a43bbc6e976d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139ed24-d020-461d-992e-c029aea50089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((128, 128)),  # Resize to a fixed size; adjust as needed\n",
    "#     transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize (mean, std) for each color channel\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900e842-7e02-44bc-aafd-7f989c546f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "\n",
    "def resize_and_crop(img):\n",
    "    # Resize while maintaining aspect ratio and center crop\n",
    "    s = min(img.size)\n",
    "    r = image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [image_size])\n",
    "    return img\n",
    "\n",
    "def modified_map_pixels(img):\n",
    "    # Add a batch dimension, apply map_pixels, and then remove the batch dimension\n",
    "    img = img.unsqueeze(0)\n",
    "    img = map_pixels(img)\n",
    "    return img.squeeze(0)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.Lambda(resize_and_crop),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(modified_map_pixels)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a29062-3951-4668-b8a8-5f088e84a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'path/to/lsun' with the actual path to your LSUN dataset\n",
    "dataset_path = './data/lsun'\n",
    "\n",
    "lsun_dataset = datasets.LSUN(root=dataset_path, classes=['bedroom_train'], transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c28ed-748a-4109-95e8-d1391e6523c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5  # Adjust based on your memory availability and requirements\n",
    "lsun_loader = DataLoader(lsun_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "print('dataset size:',len(lsun_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ae601-c58d-47d0-914f-4840124d9129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384608d8-dfd0-4f8e-a06d-2ea7309a3f2c",
   "metadata": {},
   "source": [
    "## REINFORCE Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16ec45-f652-48da-8132-0c5aff6ab98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reinforce_Loss(logits, targets, loss, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Calculate the REINFORCE loss for sequence prediction.\n",
    "\n",
    "    :param logits: Logits from the model, shape [batch_size, seq_len, vocab_size].\n",
    "    :param targets: Ground truth sequence, shape [batch_size, seq_len].\n",
    "    :param rewards: Reward for each step in the sequence, shape [batch_size, seq_len].\n",
    "    :param gamma: Discount factor for future rewards.\n",
    "    :return: The REINFORCE loss (to be maximized).\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, seq_len, _ = logits.shape\n",
    "\n",
    "    # return loss / seq_len\n",
    "    log_probs = F.log_softmax(logits, dim=2)\n",
    "    log_probs_targets = log_probs.gather(2, targets.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "    # Create a discount matrix\n",
    "    discounts = gamma ** torch.arange(seq_len).float().unsqueeze(0).to(log_probs.device)\n",
    "    discount_matrix = torch.tril(discounts.repeat(seq_len, 1).T).T\n",
    "\n",
    "\n",
    "    # Calculate discounted rewards\n",
    "    discounted_loss = loss.unsqueeze(1) * discount_matrix\n",
    "    cumulative_loss = discounted_loss.sum(dim=2)\n",
    "    \n",
    "    # Calculate loss\n",
    "    # total_loss = -torch.sum(log_probs_targets * cumulative_loss) / batch_size / seq_len\n",
    "    total_loss = torch.sum(log_probs_targets * cumulative_loss) / batch_size / seq_len\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4a481-89b6-4f0f-87b2-49b6bbeffd95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "244bb1b9-75a5-4c18-953b-1b3434bca223",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b3605-366c-4271-817d-b69ab712e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "learning_rate = 1e-4\n",
    "epochs = 1\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ecacfe-73b7-4d80-b59c-7cbcb01d60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"test\"\n",
    "exp_type = \"image\"\n",
    "experiment_name = f\"{exp_type}/{experiment}/model={llm}_lr={learning_rate}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b859448-9984-477d-a96c-a2b424627a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Create a SummaryWriter instance (logs will be saved in 'runs' folder)\n",
    "writer = SummaryWriter(f'runs/{experiment_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98894df-598d-4ad2-88e0-8c84af4904bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(mapper.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rl_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a7fcd-1692-4c3a-b24b-4e61216528a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropySG_Loss(mapped_feature_vector, targets, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Custom cross-entropy loss with straight-through estimator.\n",
    "    :return: Loss value.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, embedding_dim = mapped_feature_vector.shape\n",
    "    \n",
    "    # Closest tokens have shape [batch_size, seq_len]\n",
    "    # closest_tokens = get_llm_ground_truth(mapped_feature_vector)\n",
    "\n",
    "    closest_embeddings = embeddings[targets]\n",
    "    closest_embeddings = closest_embeddings.reshape(batch_size, seq_len, embedding_dim)\n",
    "\n",
    "\n",
    "    \n",
    "    # STE_LOGITS have shape [batch_size, seq_len, embedding_dim]\n",
    "    ste_logits = (closest_embeddings - mapped_feature_vector.detach()) + mapped_feature_vector\n",
    "\n",
    "    predictions = forward_with_embeddings(ste_logits)\n",
    "    predictions = predictions.reshape(batch_size*seq_len, -1)\n",
    "    \n",
    "    # Calculate cross-entropy loss\n",
    "    loss = F.cross_entropy(predictions, targets, reduction=reduction)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185d6601-e50a-4b95-a614-22a58b1c61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_with_encoder_(image):\n",
    "    _, z = enc(image)\n",
    "    z = np.asarray(z)\n",
    "    z = torch.from_numpy(z).to(device)\n",
    "    z_ = F.one_hot(z.long(), num_classes=imageCodebook_len).float()\n",
    "    return z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b4aca-3191-4c26-bfec-548ecab3ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce44e2de-350a-4ca7-a437-695573493cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, (images, _) in enumerate(lsun_loader):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # for vqgan\n",
    "        # images = images.permute(0, 2, 3, 1)\n",
    "        \n",
    "        # Process each image through DALL-E encoder to get image tokens\n",
    "        image_token_logits = enc(images.to(device))\n",
    "        ground_truth_tokens = torch.argmax(image_token_logits, dim=1)\n",
    "        one_hot_image_tokens = F.one_hot(ground_truth_tokens, num_classes=imageCodebook_len).permute(0, 3, 1, 2).float()\n",
    "\n",
    "        ground_truth_tokens = ground_truth_tokens.reshape( -1)\n",
    "        flattened_tokens = one_hot_image_tokens.reshape(one_hot_image_tokens.size(0), -1, imageCodebook_len)\n",
    "\n",
    "        # Map tokens and get ground truth from LLM\n",
    "        mapped_feature_vector = mapper(flattened_tokens)\n",
    "\n",
    "        translated_text_tokens = translate(mapped_feature_vector, embeddings)\n",
    "        \n",
    "        # Calculate Representation of Last Layer in LLM\n",
    "        final_layer_fv = generate_next_token_predictions(translated_text_tokens)\n",
    "\n",
    "        # Calculate Logits with mapper function\n",
    "        logits = torch.matmul(final_layer_fv, mapper.mapper.weight)\n",
    "        logits_ = logits.reshape(-1, imageCodebook_len)\n",
    "        \n",
    "        # RL Loss\n",
    "        # prediction_logits = prediction_logits.reshape(batch_size, -1, llm.vocab_len)\n",
    "        ce_loss = rl_criterion(logits_, ground_truth_tokens)\n",
    "        ground_truth_tokens = ground_truth_tokens.reshape(batch_size, -1)\n",
    "        ce_loss = ce_loss.reshape(batch_size, -1)\n",
    "\n",
    "        loss = Reinforce_Loss(logits, ground_truth_tokens, ce_loss)\n",
    "        \n",
    "        # Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the losses\n",
    "        # writer.add_scalars(\n",
    "        #     \"Training Metrics\",\n",
    "        #     {\n",
    "        #         \"loss\": loss.item(),\n",
    "        #         \"cross_entropy\": ce_loss[:,0].mean().item(),\n",
    "        #     },\n",
    "        #     epoch * len(lsun_loader) + i\n",
    "        # )\n",
    "            \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {i}, Loss: {loss.item()}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{args.epoch} completed.\")\n",
    "    \n",
    "Path(f\"models/{exp_type}/{experiment}\").mkdir(parents=True, exist_ok=True)\n",
    "torch.save(mapper.state_dict(), f\"models/{experiment_name}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d7a7e-b4f8-4f08-94b9-7c38904769c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(f\"models/{exp_type}/{experiment}\").mkdir(parents=True, exist_ok=True)\n",
    "torch.save(mapper.state_dict(), f\"models/{experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa859ab-774b-432f-a564-c0795db6bbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2448167-be48-4f48-8aaa-c1d994ba3f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqvae",
   "language": "python",
   "name": "vqvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
