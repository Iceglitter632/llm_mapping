{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366e69dd-6839-42c1-92fb-8a9766e45b19",
   "metadata": {},
   "source": [
    "## Import Needed Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1d2f0-21d1-4c06-b4ea-8b5708205a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import os, sys\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864141c-f356-49ec-a439-a855160f7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6786ef1-e246-498e-80b7-251c25259685",
   "metadata": {},
   "source": [
    "## Dalle as Image Encoder\n",
    "#### Download VQVAE from DALLE\n",
    "| testing usage\n",
    "```python\n",
    "enc = encoder\n",
    "dec = decoder\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be9b7c-2991-4007-8890-2ede65e43c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from dall_e import map_pixels, unmap_pixels, load_model\n",
    "from IPython.display import display, display_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b5f24-3259-4577-ab94-14a1ef133439",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_size = 256\n",
    "\n",
    "def download_image(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return Image.open(io.BytesIO(resp.content))\n",
    "\n",
    "def preprocess(img):\n",
    "    s = min(img.size)\n",
    "    \n",
    "    if s < target_image_size:\n",
    "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "        \n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "    return map_pixels(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf70f0-04fc-4cb3-a77f-080bf1153187",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = load_model(\"https://cdn.openai.com/dall-e/encoder.pkl\", device)\n",
    "# dec = load_model(\"https://cdn.openai.com/dall-e/decoder.pkl\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc9970-5f05-4122-9573-05b2d9e73b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = preprocess(download_image('https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iKIWgaiJUtss/v2/1000x-1.jpg'))\n",
    "display_markdown('Original image:')\n",
    "display(T.ToPILImage(mode='RGB')(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f1f3a-fe2f-4a7b-be56-21b5b4f6bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageVocab_len = enc.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfeeb78-2161-4b1e-8b06-7b495ef29cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageVocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab575dd9-c670-4b18-89dd-4f32a71e288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_with_encoder(image):\n",
    "    z = enc(image)\n",
    "    z = torch.argmax(z, axis=1)\n",
    "    z_ = F.one_hot(z, num_classes=imageVocab_len).permute(0, 3, 1, 2).float()\n",
    "    return z_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f992d-a311-4d18-b077-89fd103ebcce",
   "metadata": {},
   "source": [
    "## VQGAN as Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0559534-03de-4029-aef6-a994f39a3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vqgan_jax.modeling_flax_vqgan import VQModel\n",
    "# from transformers import VQGanForPreTraining\n",
    "# from transformers import VQGanProcessor\n",
    "\n",
    "# Load the pre-trained VQGAN model and its processor\n",
    "# checkpoint = \"dalle-mini/vqgan_imagenet_f16_16384\"\n",
    "# model = VQModel.from_pretrained(checkpoint)\n",
    "# processor = VQGanProcessor.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e2b06-9f22-4853-8055-5879bed2e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_image(url):\n",
    "#     resp = requests.get(url)\n",
    "#     resp.raise_for_status()\n",
    "#     return Image.open(io.BytesIO(resp.content))\n",
    "\n",
    "# def preprocess_vqgan(x):\n",
    "#   x = 2.*x - 1.\n",
    "#   return x\n",
    "\n",
    "# def custom_to_pil(x):\n",
    "#   x = np.clip(x, -1., 1.)\n",
    "#   x = (x + 1.)/2.\n",
    "#   x = (255*x).astype(np.uint8)\n",
    "#   x = Image.fromarray(x)\n",
    "#   if not x.mode == \"RGB\":\n",
    "#     x = x.convert(\"RGB\")\n",
    "#   return x\n",
    "\n",
    "# def preprocess(img, target_image_size=256,):\n",
    "#     s = min(img.size)\n",
    "    \n",
    "#     if s < target_image_size:\n",
    "#         raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "        \n",
    "#     r = target_image_size / s\n",
    "#     s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "#     img = TF.resize(img, s, interpolation=Image.LANCZOS)\n",
    "#     img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "#     img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "#     return img.permute(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ce53b-98e7-46cf-9ffa-ab87837d1350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from torchvision.transforms import InterpolationMode\n",
    "# def resize_image(image, size=256):\n",
    "#     s = min(image.size)\n",
    "#     r = size / s\n",
    "#     s = (round(r * image.size[1]), round(r * image.size[0]))\n",
    "#     image = TF.resize(image, s, interpolation=InterpolationMode.LANCZOS)\n",
    "#     image = TF.center_crop(image, output_size = 2 * [size])\n",
    "#     image = np.expand_dims(np.array(image), axis=0)\n",
    "#     return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8021919-8047-4232-bc01-6a971b315a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='https://heibox.uni-heidelberg.de/f/7bb608381aae4539ba7a/?dl=1'\n",
    "# size=256\n",
    "# image = download_image(url)\n",
    "# image = resize_image(image)\n",
    "# image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8792470-5521-4f88-bf30-b9d4615b879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(T.ToPILImage(mode='RGB')(image[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7cb35-e3d1-4709-bf60-bfcf106aee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, id = model.encode(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab439a5-04bd-4801-8a5a-503ed38ce773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = model.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb552fe-9f67-4216-b4db-c705b57c2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageCodebook_len = 16384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca293fa4-be2e-4d0d-9923-3be8f9288f28",
   "metadata": {},
   "source": [
    "## MIDITOK as MIDI Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6f20a-966b-4ac8-8f77-3e4b1e41e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok import REMIPlus, TokenizerConfig, REMI\n",
    "from miditoolkit import MidiFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98bb450-9744-4d85-b907-84c2d2136644",
   "metadata": {},
   "outputs": [],
   "source": [
    "PITCH_RANGE = (21, 109)\n",
    "BEAT_RES = {(0, 1): 8, (1, 2): 4, (2, 4): 2, (4, 8): 1}\n",
    "NUM_VELOCITIES = 24\n",
    "SPECIAL_TOKENS = [\"PAD\", \"MASK\", \"BOS\", \"EOS\"]\n",
    "USE_CHORDS = True\n",
    "USE_RESTS = False\n",
    "USE_TEMPOS = True\n",
    "USE_TIME_SIGNATURE = False\n",
    "USE_PROGRAMS = True\n",
    "NUM_TEMPOS = 32\n",
    "TEMPO_RANGE = (50, 200)  # (min_tempo, max_tempo)\n",
    "TOKENIZER_PARAMS = {\n",
    "    \"pitch_range\": PITCH_RANGE,\n",
    "    \"beat_res\": BEAT_RES,\n",
    "    \"num_velocities\": NUM_VELOCITIES,\n",
    "    \"special_tokens\": SPECIAL_TOKENS,\n",
    "    \"use_chords\": USE_CHORDS,\n",
    "    \"use_rests\": USE_RESTS,\n",
    "    \"use_tempos\": USE_TEMPOS,\n",
    "    \"use_time_signatures\": USE_TIME_SIGNATURE,\n",
    "    \"use_programs\": USE_PROGRAMS,\n",
    "    \"num_tempos\": NUM_TEMPOS,\n",
    "    \"tempo_range\": TEMPO_RANGE,\n",
    "}\n",
    "config = TokenizerConfig(**TOKENIZER_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f63b64-5a9f-48dc-bdc2-30c7138a1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_tokenizer = REMI(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95b91a-91e0-4c75-a9b2-5666123b003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi = MidiFile(\"../data/midi/Maestro/2004/MIDI-Unprocessed_SMF_02_R1_2004_01-05_ORIG_MID--AUDIO_02_R1_2004_05_Track05_wav.midi\")\n",
    "tokens = midi_tokenizer(midi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57218eba-5c04-4a02-a48e-a33a5a450e13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_ids = tokens.ids\n",
    "print(token_ids[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e336aae-81c9-4c02-b3eb-e9c84253fbf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "midi_vocab_len = len(midi_tokenizer.vocab)\n",
    "print(f\"midi has {midi_vocab_len} vocabularies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b1169-e09f-4fc2-b84b-0e9fae50e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_midi_tokens = F.one_hot(torch.Tensor(token_ids).long(), num_classes=midi_vocab_len)\n",
    "print(one_hot_midi_tokens.shape) # [seq_len, num_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39b2ce-583c-4c7f-bbcd-63c6092f4199",
   "metadata": {},
   "source": [
    "## Text LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b9076-f123-4ad3-9d3b-a4fe1f6af9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# llm = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model = LlamaForCausalLM.from_pretrained(llm)\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9b5f8-0a99-4d08-9a29-7ea1456b5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "llm = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(llm)\n",
    "llm_tokenizer = GPT2Tokenizer.from_pretrained(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205e7cb-7b7d-4280-bc7e-4f97f3de6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.lm_head.weight\n",
    "# embedding_matrix = model.transformer.wte.weight\n",
    "llm_feature_dim = model.config.hidden_size\n",
    "llm_vocab_len = model.config.vocab_size\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469ea76-4126-463c-84b4-60ae60a640e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = embeddings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8413d-4467-4f4a-b1f4-ff89f690fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gpt2 feature dim length:\", llm_feature_dim)\n",
    "print(\"gpt2 vocabulary length:\", llm_vocab_len)\n",
    "print(\"gpt2 embedding shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfcd152-c0b7-4ee2-b23f-71ea3ad7a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_llm_with_embeddings(embeddings):\n",
    "    \"\"\"\n",
    "    Forward pass through GPT-2 for sequential token prediction logits from embeddings.\n",
    "\n",
    "    :param embeddings: Embeddings of the sequence, shape [batch_size, seq_len, embedding_dim].\n",
    "    :return: Tensor of logits for token predictions, shape [batch_size, seq_len, vocab_size].\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = embeddings.size()\n",
    "    vocab_size = model.config.vocab_size\n",
    "    predicted_logits = torch.zeros((batch_size, seq_len, vocab_size), device=embeddings.device)\n",
    "    \n",
    "    gpt2_model.eval()\n",
    "\n",
    "    embeddings = embeddings.detach()\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        # Use embeddings up to the i-th position to predict the next token\n",
    "        input_embeddings = embeddings[:, :i+1, :]\n",
    "\n",
    "        # Forward pass through GPT-2\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt2_model(inputs_embeds=input_embeddings)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Get the logits for the next position (i+1)\n",
    "        predicted_logits[:, i, :] = logits[:, -1, :]  # Last token in the sequence\n",
    "\n",
    "    predicted_logits.requires_grad = True\n",
    "\n",
    "    return predicted_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e68f45-f220-4f68-99b4-2ec4f4e93803",
   "metadata": {},
   "source": [
    "## Mapper Network\n",
    "\n",
    "map some modality to text token's feature dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d24430-98fe-4cab-8d5f-387d00094e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74b816-7d75-4b25-91ec-7307266912af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenMapper(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.mapper = nn.Linear(input_dim, output_dim)\n",
    "        self.mapper.to(device)\n",
    "\n",
    "    def forward(self, one_hot_token):\n",
    "        return self.mapper(one_hot_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ca007-4c83-4196-b5a1-ad7bf2c2a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapper\n",
    "# mapper maps vocabulary_size of target modality to feature_dimension size of llm\n",
    "# mapper = TokenMapper(midi_vocab_len, llm_feature_dim, device=device)\n",
    "mapper = TokenMapper(imageVocab_len, llm_feature_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb20d95-9c20-4d00-92c1-531ab718190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afdbe0-2789-4c1a-b2b4-792cc84fd05d",
   "metadata": {},
   "source": [
    "## Generate Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298cf65-3f3a-41a6-8edb-f3c1cba88ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token_predictions(token_sequences):\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=token_sequences, output_hidden_states=True)\n",
    "    \n",
    "    return outputs.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad186c-171a-419c-9d6a-3f2a39e4cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_token_logits(batch_feature_vectors, embeddings):\n",
    "        \"\"\"\n",
    "        Find the LLM token who has the highest dot product to the given feature vector\n",
    "        This acts as the action of our REINFORCE algorithm\n",
    "        \n",
    "        return prob: Probability of each token getting chosen => shape:(batch_size, seq_len, llm_vocabulary_size)\n",
    "        return closest_tokens: The token with the highest probability => shape:(batch_size, seq_len) \n",
    "        \"\"\"\n",
    "        dot_product = torch.matmul(batch_feature_vectors, embeddings.T)\n",
    "        probs = F.softmax(dot_product, dim=-1)\n",
    "        closest_tokens = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        return closest_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf798ea2-42a7-4c26-b4d6-e67ea61cc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth(mapped_feature_vector, embeddings):\n",
    "    \n",
    "    ground_truth = find_closest_token_logits(mapped_feature_vector, embeddings)\n",
    "\n",
    "    return ground_truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37290d-4b1c-4c74-8f28-378382b80b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(batch_feature_vectors, embeddings):\n",
    "    batch_size, seq_len, embedding_dim = batch_feature_vectors.shape\n",
    "    closest_tokens = torch.zeros((batch_size, seq_len), dtype=torch.long)\n",
    "\n",
    "    # Normalize the embedding matrix\n",
    "    embedding_matrix_norm = F.normalize(embeddings, dim=1)\n",
    "\n",
    "    closest_tokens = torch.zeros((batch_size, seq_len), dtype=torch.long).to(device)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Normalize the feature vectors for the i-th sample in the batch\n",
    "        feature_vectors_norm = F.normalize(batch_feature_vectors[i], dim=1)\n",
    "\n",
    "        # Compute cosine similarity for the entire sequence at once\n",
    "        cosine_similarities = torch.matmul(feature_vectors_norm, embedding_matrix_norm.T)\n",
    "\n",
    "        # Find the token with the highest similarity for each feature vector\n",
    "        closest_tokens[i] = torch.argmax(cosine_similarities, dim=1)\n",
    "\n",
    "    return closest_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec8daa-a7c4-461d-8cfd-391a4d0287d9",
   "metadata": {},
   "source": [
    "## Get Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c1e6e-dbb0-43cf-84d1-a43bbc6e976d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139ed24-d020-461d-992e-c029aea50089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((128, 128)),  # Resize to a fixed size; adjust as needed\n",
    "#     transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize (mean, std) for each color channel\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900e842-7e02-44bc-aafd-7f989c546f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "\n",
    "def resize_and_crop(img):\n",
    "    # Resize while maintaining aspect ratio and center crop\n",
    "    s = min(img.size)\n",
    "    r = image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [image_size])\n",
    "    return img\n",
    "\n",
    "def modified_map_pixels(img):\n",
    "    # Add a batch dimension, apply map_pixels, and then remove the batch dimension\n",
    "    img = img.unsqueeze(0)\n",
    "    img = map_pixels(img)\n",
    "    return img.squeeze(0)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.Lambda(resize_and_crop),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(modified_map_pixels)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a29062-3951-4668-b8a8-5f088e84a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'path/to/lsun' with the actual path to your LSUN dataset\n",
    "dataset_path = '../data/lsun'\n",
    "\n",
    "lsun_dataset = datasets.LSUN(root=dataset_path, classes=['bedroom_train'], transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c28ed-748a-4109-95e8-d1391e6523c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20  # Adjust based on your memory availability and requirements\n",
    "lsun_loader = DataLoader(lsun_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "print('dataset size:',len(lsun_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ae601-c58d-47d0-914f-4840124d9129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e62379-a2fc-4d28-a781-24f7665bab23",
   "metadata": {},
   "source": [
    "# Get Midi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62bd9bd-be0a-4d21-a219-46f879cea582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok.pytorch_data.datasets import DatasetTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86d5c9-273f-4432-95df-d5c5354a1890",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"../data/midi/MMD_MIDI\")\n",
    "tokens_path = Path(\"../data/midi/MMD_MIDI_no_bpe\")\n",
    "pattern = re.compile(r\"/\\._\")\n",
    "\n",
    "# Use glob to find all .mid files and filter out the undesired ones\n",
    "midi_files = [file for file in dataset_path.glob(\"**/*.mid\") if not pattern.search(str(file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2148b9b7-9f88-4d2a-af3e-fc52e3a38273",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_paths = list(Path('../data/midi/Maestro').glob('**/*.mid')) + list(Path('../data/midi/Maestro').glob('**/*.midi'))\n",
    "tokens_path = Path('../data/midi/Maestro_tokens_no_bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8610a-d79e-475d-9efa-a28002c2f3da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for f in midi_files:\n",
    "#     print(f)\n",
    "#     midi_tokenizer.tokenize_midi_dataset(f, tokens_path, logging=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62db3b-5eb2-4668-beca-26d31997b4bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "midi_tokenizer.tokenize_midi_dataset(midi_paths, tokens_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc6192e-fa2b-4c6c-a7e9-2f2ad0a47f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_bpe_path = Path('../data/midi/Maestro_tokens_bpe')\n",
    "tokens_bpe_path.mkdir(exist_ok=True, parents=True)\n",
    "midi_tokenizer.learn_bpe(\n",
    "    vocab_size=10000,\n",
    "    tokens_paths=list(tokens_path.glob(\"**/*.json\")),\n",
    "    start_from_empty_voc=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9128b0c-7414-44e5-ac7b-d9b36dc0c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_tokenizer.save_params(\"tokenizer_bpe.conf\")\n",
    "midi_tokenizer.apply_bpe_to_dataset(\n",
    "    tokens_path,\n",
    "    tokens_bpe_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5adfea-c40d-4e74-b97a-658ead66a739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27260cd-52e6-475c-ab00-486e96c858a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_paths = list(Path('../data/midi/Maestro_tokens_no_bpe').glob(\"**/*.json\"))\n",
    "\n",
    "midi_dataset = DatasetTok(\n",
    "    tokens_paths, max_seq_len=128, min_seq_len=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c150f5b-8195-46f3-904f-8ac40953fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(midi_dataset)*0.8)\n",
    "val_size = len(midi_dataset) - train_size\n",
    "\n",
    "train_dataset, validation_dataset = random_split(midi_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb461da0-c11b-4c2a-858e-7eb24250fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "midi_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3894a81f-dd07-4298-92c8-a1e32fecc9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_midi_dataset(\n",
    "    midi_paths,\n",
    "    pitch_offsets=[-12, 12],\n",
    "    velocity_offsets=[-4, 5],\n",
    "    duration_offsets=[-0.5, 1],\n",
    "    out_path=midi_aug_path,\n",
    "    Path(\"to\", \"new\", \"location\", \"augmented\"),\n",
    ")\n",
    "tokenizer.tokenize_midi_dataset(        # 2 velocity and 1 duration values\n",
    "    midi_paths,\n",
    "    Path(\"path\", \"to\", \"tokens\"),\n",
    "    midi_valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384608d8-dfd0-4f8e-a06d-2ea7309a3f2c",
   "metadata": {},
   "source": [
    "## REINFORCE Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16ec45-f652-48da-8132-0c5aff6ab98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reinforce_Loss(logits, translated, loss, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Calculate the REINFORCE loss for sequence prediction.\n",
    "\n",
    "    :param logits: Logits from the model, shape [batch_size, seq_len, vocab_size].\n",
    "    :param targets: Ground truth sequence, shape [batch_size, seq_len].\n",
    "    :param rewards: Reward for each step in the sequence, shape [batch_size, seq_len].\n",
    "    :param gamma: Discount factor for future rewards.\n",
    "    :return: The REINFORCE loss (to be maximized).\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    batch_size, seq_len, _ = logits.shape\n",
    "\n",
    "    # shape = [batch_size, seq_len, llm_vocab_len]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    log_probs_targets = log_probs.gather(2, translated.unsqueeze(2)).squeeze(2)\n",
    "    \n",
    "    # Create a discount matrix\n",
    "    discount_matrix = torch.zeros((seq_len, seq_len)).to(device)\n",
    "\n",
    "    # Fill the matrix according to the given pattern\n",
    "    for i in range(seq_len):\n",
    "        for j in range(i, seq_len):\n",
    "            discount_matrix[i, j] = gamma ** (j - i)\n",
    "    \n",
    "    # Calculate discounted rewards\n",
    "    discounted_loss = loss.unsqueeze(1) * discount_matrix\n",
    "    cumulative_loss = discounted_loss.sum(dim=-1)\n",
    "    \n",
    "    # Calculate loss\n",
    "    total_loss = torch.sum(log_probs_targets * cumulative_loss.detach()) / batch_size / seq_len\n",
    "    # total_loss = torch.sum(log_probs_targets * cumulative_loss) / batch_size / seq_len\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4a481-89b6-4f0f-87b2-49b6bbeffd95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "244bb1b9-75a5-4c18-953b-1b3434bca223",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b3605-366c-4271-817d-b69ab712e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "learning_rate = 1e-5\n",
    "epochs = 1\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ecacfe-73b7-4d80-b59c-7cbcb01d60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"rl\"\n",
    "exp_type = \"image_test\"\n",
    "experiment_name = f\"{exp_type}/{experiment}/model={llm}_lr={learning_rate}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b859448-9984-477d-a96c-a2b424627a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Create a SummaryWriter instance (logs will be saved in 'runs' folder)\n",
    "writer = SummaryWriter(log_dir = f'../runs/{experiment_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98894df-598d-4ad2-88e0-8c84af4904bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(mapper.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rl_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a7fcd-1692-4c3a-b24b-4e61216528a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def CrossEntropySG_Loss(mapped_feature_vector, targets, reduction='mean'):\n",
    "#     \"\"\"\n",
    "#     Custom cross-entropy loss with straight-through estimator.\n",
    "#     :return: Loss value.\n",
    "#     \"\"\"\n",
    "#     batch_size, seq_len, embedding_dim = mapped_feature_vector.shape\n",
    "    \n",
    "#     # Closest tokens have shape [batch_size, seq_len]\n",
    "#     # closest_tokens = get_llm_ground_truth(mapped_feature_vector)\n",
    "\n",
    "#     closest_embeddings = embeddings[targets]\n",
    "#     closest_embeddings = closest_embeddings.reshape(batch_size, seq_len, embedding_dim)\n",
    "\n",
    "\n",
    "    \n",
    "#     # STE_LOGITS have shape [batch_size, seq_len, embedding_dim]\n",
    "#     ste_logits = (closest_embeddings - mapped_feature_vector.detach()) + mapped_feature_vector\n",
    "\n",
    "#     predictions = forward_with_embeddings(ste_logits)\n",
    "#     predictions = predictions.reshape(batch_size*seq_len, -1)\n",
    "    \n",
    "#     # Calculate cross-entropy loss\n",
    "#     loss = F.cross_entropy(predictions, targets, reduction=reduction)\n",
    "\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185d6601-e50a-4b95-a614-22a58b1c61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_with_encoder_(image):\n",
    "    _, z = enc(image)\n",
    "    z = np.asarray(z)\n",
    "    z = torch.from_numpy(z).to(device)\n",
    "    z_ = F.one_hot(z.long(), num_classes=imageCodebook_len).float()\n",
    "    return z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b4aca-3191-4c26-bfec-548ecab3ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2fb2e-0a55-4525-8c72-51543e1724d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    mapper.train()\n",
    "    for i, midi in enumerate(midi_loader):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # midi_one_hot shape -> [batch_size, seq_len, feature_dim]\n",
    "        ground_truth_tokens = midi[\"input_ids\"]\n",
    "        one_hot_midi_tokens = F.one_hot(ground_truth_tokens, num_classes=midi_vocab_len).float().to(device)\n",
    "\n",
    "        ground_truth_tokens = ground_truth_tokens.reshape(-1).to(device)\n",
    "\n",
    "        # Map tokens and get ground truth from LLM\n",
    "        mapped_feature_vector = mapper(one_hot_midi_tokens)\n",
    "\n",
    "        translated_text_tokens = translate(mapped_feature_vector, embeddings)\n",
    "        \n",
    "        # Calculate Representation of Last Layer in LLM\n",
    "        final_layer_fv = generate_next_token_predictions(translated_text_tokens)\n",
    "\n",
    "        # Calculate Logits with mapper function\n",
    "        logits = torch.matmul(final_layer_fv, mapper.mapper.weight)\n",
    "        logits_ = logits.reshape(-1, midi_vocab_len)\n",
    "\n",
    "        # loss = criterion(logits_, ground_truth_tokens)\n",
    "        \n",
    "        # RL Loss\n",
    "        ce_loss = rl_criterion(logits_, ground_truth_tokens)\n",
    "        ground_truth_tokens = ground_truth_tokens.reshape(batch_size, -1)\n",
    "        ce_loss = ce_loss.reshape(batch_size, -1)\n",
    "\n",
    "        loss = Reinforce_Loss(logits, translated_text_tokens, ce_loss)\n",
    "        \n",
    "        # Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the losses\n",
    "        writer.add_scalars(\n",
    "            \"Training Metrics\",\n",
    "            {\n",
    "                \"loss\": loss.item(),\n",
    "                \"cross_entropy\": ce_loss.mean().item(),\n",
    "            },\n",
    "            epoch * len(midi_loader) + i\n",
    "        )\n",
    "        # writer.add_scalar(\"training/cross_entropy\", loss.item(), epoch*len(midi_loader)+i)\n",
    "            \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {i}, Loss: {loss.item()}\")\n",
    "\n",
    "    mapper.eval()\n",
    "    print(\"Start Testing\")\n",
    "    for i, midi in enumerate(val_loader):\n",
    "        \n",
    "        ground_truth_tokens = midi[\"input_ids\"]\n",
    "        one_hot_midi_tokens = F.one_hot(ground_truth_tokens, num_classes=midi_vocab_len).float().to(device)\n",
    "\n",
    "        ground_truth_tokens = ground_truth_tokens.reshape(-1).to(device)\n",
    "\n",
    "        # Map tokens and get ground truth from LLM\n",
    "        mapped_feature_vector = mapper(one_hot_midi_tokens)\n",
    "\n",
    "        translated_text_tokens = translate(mapped_feature_vector, embeddings)\n",
    "        \n",
    "        # Calculate Representation of Last Layer in LLM\n",
    "        final_layer_fv = generate_next_token_predictions(translated_text_tokens)\n",
    "\n",
    "        # Calculate Logits with mapper function\n",
    "        logits = torch.matmul(final_layer_fv, mapper.mapper.weight)\n",
    "        logits_ = logits.reshape(-1, midi_vocab_len)\n",
    "\n",
    "        loss = criterion(logits_, ground_truth_tokens)\n",
    "        \n",
    "        # RL Loss\n",
    "        ce_loss = rl_criterion(logits_, ground_truth_tokens)\n",
    "        ground_truth_tokens = ground_truth_tokens.reshape(batch_size, -1)\n",
    "        ce_loss = ce_loss.reshape(batch_size, -1)\n",
    "\n",
    "        loss = Reinforce_Loss(logits, ground_truth_tokens, ce_loss)\n",
    "        \n",
    "        # Backward pass and update\n",
    "        # Log the losses\n",
    "        writer.add_scalars(\n",
    "            \"Training Metrics\",\n",
    "            {\n",
    "                \"loss\": loss.item(),\n",
    "                \"cross_entropy\": ce_loss.mean().item(),\n",
    "            },\n",
    "            epoch * len(val_loader) + i\n",
    "        )\n",
    "        # writer.add_scalar(\"validation/cross_entropy\", loss.item(), epoch*len(val_loader)+i)\n",
    "            \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {i}, Loss: {loss.item()}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0c8bd-bd0f-4d58-a50a-98a0ac8b6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f\"../models/{exp_type}/{experiment}\").mkdir(parents=True, exist_ok=True)\n",
    "torch.save(mapper.state_dict(), f\"../models/{experiment_name}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce44e2de-350a-4ca7-a437-695573493cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, (images, _) in enumerate(lsun_loader):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # for vqgan\n",
    "        # images = images.permute(0, 2, 3, 1)\n",
    "        \n",
    "        # Process each image through DALL-E encoder to get image tokens\n",
    "        image_token_logits = enc(images.to(device))\n",
    "        ground_truth_tokens = torch.argmax(image_token_logits, dim=1)\n",
    "        one_hot_image_tokens = F.one_hot(ground_truth_tokens, num_classes=imageVocab_len).permute(0, 3, 1, 2).float()\n",
    "\n",
    "        ground_truth_tokens = ground_truth_tokens.reshape( -1)\n",
    "        flattened_tokens = one_hot_image_tokens.reshape(one_hot_image_tokens.size(0), -1, imageVocab_len)\n",
    "\n",
    "        # Map tokens and get ground truth from LLM\n",
    "        mapped_feature_vector = mapper(flattened_tokens)\n",
    "\n",
    "        translated_text_tokens = translate(mapped_feature_vector, embeddings)\n",
    "        \n",
    "        # Calculate Representation of Last Layer in LLM\n",
    "        final_layer_fv = generate_next_token_predictions(translated_text_tokens)\n",
    "\n",
    "        # Calculate Logits with mapper function\n",
    "        logits = torch.matmul(final_layer_fv, mapper.mapper.weight)\n",
    "        logits_ = logits.reshape(-1, imageVocab_len)\n",
    "\n",
    "        # shape = [batch_size, seq_len, llm_vocab_len]\n",
    "        action_logits = torch.matmul(mapped_feature_vector, embeddings.T)\n",
    "        # RL Loss\n",
    "        # prediction_logits = prediction_logits.reshape(batch_size, -1, llm.vocab_len)\n",
    "        loss = criterion(logits_, ground_truth_tokens)\n",
    "        ce_loss = rl_criterion(logits_, ground_truth_tokens)\n",
    "        ground_truth_tokens = ground_truth_tokens.reshape(batch_size, -1)\n",
    "        ce_loss = ce_loss.reshape(batch_size, -1)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss = Reinforce_Loss(action_logits, translated_text_tokens, ce_loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Backward pass and update\n",
    "        \n",
    "\n",
    "        # Log the losses\n",
    "        # writer.add_scalars(\n",
    "        #     \"Training Metrics\",\n",
    "        #     {\n",
    "        #         \"loss\": loss.item(),\n",
    "        #         \"cross_entropy\": ce_loss[:,0].mean().item(),\n",
    "        #     },\n",
    "        #     epoch * len(lsun_loader) + i\n",
    "        # )\n",
    "            \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {i}, Loss: {loss.item()}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed.\")\n",
    "    \n",
    "Path(f\"models/{exp_type}/{experiment}\").mkdir(parents=True, exist_ok=True)\n",
    "torch.save(mapper.state_dict(), f\"models/{experiment_name}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d7a7e-b4f8-4f08-94b9-7c38904769c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(f\"models/{exp_type}/{experiment}\").mkdir(parents=True, exist_ok=True)\n",
    "torch.save(mapper.state_dict(), f\"models/{experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa859ab-774b-432f-a564-c0795db6bbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2448167-be48-4f48-8aaa-c1d994ba3f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqvae",
   "language": "python",
   "name": "vqvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
