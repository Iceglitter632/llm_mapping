{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366e69dd-6839-42c1-92fb-8a9766e45b19",
   "metadata": {},
   "source": [
    "## Import Needed Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac1d2f0-21d1-4c06-b4ea-8b5708205a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import requests\n",
    "import numpy as np\n",
    " \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1864141c-f356-49ec-a439-a855160f7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d8057b-5c43-4d55-9641-caf91f0de4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7df7aae6f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(41648)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca293fa4-be2e-4d0d-9923-3be8f9288f28",
   "metadata": {},
   "source": [
    "## MIDITOK as MIDI Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4d6f20a-966b-4ac8-8f77-3e4b1e41e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok import REMIPlus, TokenizerConfig, REMI\n",
    "from miditoolkit import MidiFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a98bb450-9744-4d85-b907-84c2d2136644",
   "metadata": {},
   "outputs": [],
   "source": [
    "PITCH_RANGE = (21, 109)\n",
    "BEAT_RES = {(0, 1): 8, (1, 2): 4, (2, 4): 2, (4, 8): 1}\n",
    "NUM_VELOCITIES = 24\n",
    "SPECIAL_TOKENS = [\"PAD\", \"MASK\", \"BOS\", \"EOS\"]\n",
    "USE_CHORDS = True\n",
    "USE_RESTS = False\n",
    "USE_TEMPOS = True\n",
    "USE_TIME_SIGNATURE = False\n",
    "USE_PROGRAMS = True\n",
    "NUM_TEMPOS = 32\n",
    "TEMPO_RANGE = (50, 200)  # (min_tempo, max_tempo)\n",
    "TOKENIZER_PARAMS = {\n",
    "    \"pitch_range\": PITCH_RANGE,\n",
    "    \"beat_res\": BEAT_RES,\n",
    "    \"num_velocities\": NUM_VELOCITIES,\n",
    "    \"special_tokens\": SPECIAL_TOKENS,\n",
    "    \"use_chords\": USE_CHORDS,\n",
    "    \"use_rests\": USE_RESTS,\n",
    "    \"use_tempos\": USE_TEMPOS,\n",
    "    \"use_time_signatures\": USE_TIME_SIGNATURE,\n",
    "    \"use_programs\": USE_PROGRAMS,\n",
    "    \"num_tempos\": NUM_TEMPOS,\n",
    "    \"tempo_range\": TEMPO_RANGE,\n",
    "}\n",
    "config = TokenizerConfig(**TOKENIZER_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f63b64-5a9f-48dc-bdc2-30c7138a1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_tokenizer = REMI(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d95b91a-91e0-4c75-a9b2-5666123b003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi = MidiFile(\"../data/midi/Maestro/2004/MIDI-Unprocessed_SMF_02_R1_2004_01-05_ORIG_MID--AUDIO_02_R1_2004_05_Track05_wav.midi\")\n",
    "tokens = midi_tokenizer(midi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e336aae-81c9-4c02-b3eb-e9c84253fbf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midi has 344 vocabularies\n"
     ]
    }
   ],
   "source": [
    "midi_vocab_len = len(midi_tokenizer.vocab)\n",
    "print(f\"midi has {midi_vocab_len} vocabularies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a4b1169-e09f-4fc2-b84b-0e9fae50e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_midi_tokens = F.one_hot(torch.Tensor(token_ids).long(), num_classes=midi_vocab_len)\n",
    "# print(one_hot_midi_tokens.shape) # [seq_len, num_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39b2ce-583c-4c7f-bbcd-63c6092f4199",
   "metadata": {},
   "source": [
    "## Text LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4b9076-f123-4ad3-9d3b-a4fe1f6af9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# llm = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model = LlamaForCausalLM.from_pretrained(llm)\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4a9b5f8-0a99-4d08-9a29-7ea1456b5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "llm = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(llm)\n",
    "llm_tokenizer = GPT2Tokenizer.from_pretrained(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f205e7cb-7b7d-4280-bc7e-4f97f3de6048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = model.lm_head.weight\n",
    "# embedding_matrix = model.transformer.wte.weight\n",
    "llm_feature_dim = model.config.hidden_size\n",
    "llm_vocab_len = model.config.vocab_size\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab408c45-a8ae-4624-819f-b08e479ebe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f469ea76-4126-463c-84b4-60ae60a640e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = embeddings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bae8413d-4467-4f4a-b1f4-ff89f690fd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 feature dim length: 768\n",
      "gpt2 vocabulary length: 50257\n",
      "gpt2 embedding shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"gpt2 feature dim length:\", llm_feature_dim)\n",
    "print(\"gpt2 vocabulary length:\", llm_vocab_len)\n",
    "print(\"gpt2 embedding shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e68f45-f220-4f68-99b4-2ec4f4e93803",
   "metadata": {},
   "source": [
    "## Mapper Network\n",
    "\n",
    "map some modality to text token's feature dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36d24430-98fe-4cab-8d5f-387d00094e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff91bda6-68f2-4548-b4af-ad909bed07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY FOR BPE\n",
    "# midi_vocab_len = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e74b816-7d75-4b25-91ec-7307266912af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenMapper(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.mapper = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.mapper.to(device)\n",
    "\n",
    "    def forward(self, one_hot_token):\n",
    "        return self.mapper(one_hot_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b93ca007-4c83-4196-b5a1-ad7bf2c2a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapper\n",
    "# mapper maps vocabulary_size of target modality to feature_dimension size of llm\n",
    "# mapper = TokenMapper(midi_vocab_len, llm_feature_dim, device=device)\n",
    "mapper = TokenMapper(midi_vocab_len, llm_feature_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fb20d95-9c20-4d00-92c1-531ab718190c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenMapper(\n",
       "  (mapper): Linear(in_features=344, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e0e782a-5e1d-4c2a-a5f2-e98470680320",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverseMapper = TokenMapper(midi_vocab_len, llm_feature_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f0704f5-85e6-4cc0-af1a-bebf2c0a4b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenMapper(\n",
       "  (mapper): Linear(in_features=344, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverseMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c562e-08c0-48aa-b82b-ea6c28bfb99a",
   "metadata": {},
   "source": [
    "## Prompt Network\n",
    "give some prompts for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f9fbaf3-3bf3-4be0-bf4f-9012076e32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c169a11-b403-4691-8eed-a4a01f2ee04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def forward(self, one_hot_token):\n",
    "        return self.model(one_hot_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f43cf054-9229-4eea-b2b3-fc320fa45acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prompt_len!=0:\n",
    "    prompt = Prompt(prompt_len, llm_feature_dim, device=device)\n",
    "    prompt_inputs = F.one_hot(torch.arange(prompt_len), num_classes=prompt_len).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afdbe0-2789-4c1a-b2b4-792cc84fd05d",
   "metadata": {},
   "source": [
    "## Generate Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6298cf65-3f3a-41a6-8edb-f3c1cba88ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token_predictions(token_sequences):\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=token_sequences, output_hidden_states=True)\n",
    "    \n",
    "    return outputs.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8251351-5d0e-43b5-8805-848acdc7ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token_predictions_withfv(token_fv):\n",
    "    \n",
    "    # Get model predictions\n",
    "    outputs = model(inputs_embeds=token_fv, output_hidden_states=True)\n",
    "    \n",
    "    return outputs.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1bbdbb-1ab0-465a-bde2-c7d80289ec25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2f328-5ff9-44d6-863c-33a27580bb60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c37290d-4b1c-4c74-8f28-378382b80b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(batch_feature_vectors, embeddings, temperature=1.0):\n",
    "    batch_size, seq_len, embedding_dim = batch_feature_vectors.shape\n",
    "\n",
    "    # Normalize the embedding matrix\n",
    "    # embedding_matrix_norm = F.normalize(embeddings, dim=1)\n",
    "\n",
    "    # batch_feature_vector_norm = F.normalize(batch_feature_vectors, dim=2)\n",
    "    # cosine_similarities = torch.matmul(batch_feature_vector_norm, embedding_matrix_norm.T)\n",
    "    cosine_similarities = torch.matmul(batch_feature_vectors, embeddings.T)\n",
    "    sfmx = torch.softmax(cosine_similarities/temperature, dim=2)\n",
    "    closest_tokens = torch.argmax(sfmx, dim=2)\n",
    "    \n",
    "    mm = torch.matmul(sfmx, embeddings)\n",
    "    # closest_tokens1 = torch.zeros((batch_size, seq_len), dtype=torch.float).to(device)\n",
    "    # mm1 = torch.zeros((batch_size, seq_len, embeddings.size(1)), dtype=torch.float).to(device)\n",
    "    \n",
    "    # for i in range(batch_size):\n",
    "    #     # Normalize the feature vectors for the i-th sample in the batch\n",
    "    #     feature_vectors_norm = F.normalize(batch_feature_vectors[i], dim=1)\n",
    "\n",
    "    #     # Compute cosine similarity for the entire sequence at once\n",
    "    #     cosine_similarities = torch.matmul(feature_vectors_norm, embedding_matrix_norm.T)\n",
    "\n",
    "    #     # Find the token with the highest similarity for each feature vector\n",
    "    #     closest_tokens1[i] = torch.argmax(cosine_similarities, dim=1)\n",
    "\n",
    "    #     mm1[i] = torch.matmul(torch.softmax(cosine_similarities / temperature, dim=1), embeddings)\n",
    "\n",
    "\n",
    "    return mm, cosine_similarities, closest_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e62379-a2fc-4d28-a781-24f7665bab23",
   "metadata": {},
   "source": [
    "# Get Midi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d62bd9bd-be0a-4d21-a219-46f879cea582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok.pytorch_data.datasets import DatasetTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec86d5c9-273f-4432-95df-d5c5354a1890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = Path(\"../data/midi/MMD_MIDI\")\n",
    "# tokens_path = Path(\"../data/midi/MMD_MIDI_no_bpe\")\n",
    "# pattern = re.compile(r\"/\\._\")\n",
    "\n",
    "# # Use glob to find all .mid files and filter out the undesired ones\n",
    "# midi_files = [file for file in dataset_path.glob(\"**/*.mid\") if not pattern.search(str(file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2148b9b7-9f88-4d2a-af3e-fc52e3a38273",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_paths = list(Path('../data/midi/Maestro').glob('**/*.mid')) + list(Path('../data/midi/Maestro').glob('**/*.midi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d1d41e6-a6a6-4f63-b0e5-3418ce03fa12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../data/midi/Maestro/2009/MIDI-Unprocessed_15_R1_2009_03-06_ORIG_MID--AUDIO_15_R1_2009_15_R1_2009_06_WAV.midi')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e62db3b-5eb2-4668-beca-26d31997b4bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# midi_tokenizer.tokenize_midi_dataset(midi_paths, tokens_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cc6192e-fa2b-4c6c-a7e9-2f2ad0a47f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_path = Path('../data/midi/Maestro_tokens_no_bpe')\n",
    "# tokens_bpe_path = Path('../data/midi/Maestro_tokens_bpe')\n",
    "# tokens_bpe_path.mkdir(exist_ok=True, parents=True)\n",
    "# midi_tokenizer.learn_bpe(\n",
    "#     vocab_size=10000,\n",
    "#     tokens_paths=list(tokens_path.glob(\"**/*.json\")),\n",
    "#     start_from_empty_voc=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9128b0c-7414-44e5-ac7b-d9b36dc0c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi_tokenizer.save_params(\"tokenizer_bpe.conf\")\n",
    "# midi_tokenizer.apply_bpe_to_dataset(\n",
    "#     tokens_path,\n",
    "#     tokens_bpe_path,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb5adfea-c40d-4e74-b97a-658ead66a739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokens_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b1d2abd-83f0-47b1-9e9e-fa5d0b0b2a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: ../data/midi/Maestro_tokens_no_bpe/2009: 100%|███████████████████████| 1276/1276 [00:03<00:00, 323.94it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens_paths = list(Path('../data/midi/Maestro_tokens_no_bpe').glob(\"**/*.json\"))\n",
    "# tokens_paths = list(Path('../data/midi/Maestro_tokens_bpe').glob(\"**/*.json\"))\n",
    "midi_dataset = DatasetTok(\n",
    "    tokens_paths, max_seq_len=128, min_seq_len=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c150f5b-8195-46f3-904f-8ac40953fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(midi_dataset)*0.8)\n",
    "val_size = len(midi_dataset) - train_size\n",
    "\n",
    "train_dataset, validation_dataset = random_split(midi_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb461da0-c11b-4c2a-858e-7eb24250fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "midi_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384608d8-dfd0-4f8e-a06d-2ea7309a3f2c",
   "metadata": {},
   "source": [
    "## REINFORCE Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d16ec45-f652-48da-8132-0c5aff6ab98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reinforce_Loss(logits, translated, loss, gamma=0.9, alpha=1, temperature=1):\n",
    "    \"\"\"\n",
    "    Calculate the REINFORCE loss for sequence prediction.\n",
    "\n",
    "    :param logits: Logits from the model, shape [batch_size, seq_len, vocab_size].\n",
    "    :param targets: Ground truth sequence, shape [batch_size, seq_len].\n",
    "    :param rewards: Reward for each step in the sequence, shape [batch_size, seq_len].\n",
    "    :param gamma: Discount factor for future rewards.\n",
    "    :return: The REINFORCE loss (to be maximized).\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = logits.shape\n",
    "    translated = translated.to(torch.int64)\n",
    "    # shape = [batch_size, seq_len, llm_vocab_len]\n",
    "    log_probs = F.log_softmax(logits/temperature, dim=-1)\n",
    "    log_probs_targets = log_probs.gather(2, translated.unsqueeze(2)).squeeze(2)\n",
    "    \n",
    "    # Create a discount matrix\n",
    "    discount_matrix = torch.zeros((seq_len, seq_len)).to(device)\n",
    "\n",
    "    # Fill the matrix according to the given pattern\n",
    "    for i in range(seq_len):\n",
    "        for j in range(i, seq_len):\n",
    "            discount_matrix[i, j] = gamma ** (j - i)\n",
    "\n",
    "    normalize_factor = discount_matrix.sum(dim=1)\n",
    "    \n",
    "    # Calculate discounted rewards\n",
    "    discounted_loss = loss.unsqueeze(1) * discount_matrix\n",
    "    cumulative_loss = discounted_loss.sum(dim=-1) / normalize_factor / alpha\n",
    "    \n",
    "    # Calculate loss\n",
    "    total_loss = torch.sum(log_probs_targets * cumulative_loss) / batch_size / seq_len\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244bb1b9-75a5-4c18-953b-1b3434bca223",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "870b3605-366c-4271-817d-b69ab712e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "learning_rate = 1e-5\n",
    "epochs = 1\n",
    "gamma = 0.1\n",
    "temperature = 0.01\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0ecacfe-73b7-4d80-b59c-7cbcb01d60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"testing_nobpe_dual\"\n",
    "algo = \"rl\"\n",
    "exp_type = \"midi\"\n",
    "name = \"remi\"\n",
    "experiment_name = f\"{exp_type}/{algo}/{experiment}/{name}/{llm}/lr={learning_rate},gamma={gamma},temp={temperature},promptlen={prompt_len}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28b822f9-3665-4ceb-afb1-56fc0034a63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'midi/testing_nobpe_dual/remi/gpt2/lr=1e-05,gamma=0.1,temp=0.01,promptlen=0'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b859448-9984-477d-a96c-a2b424627a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # Create a SummaryWriter instance (logs will be saved in 'runs' folder)\n",
    "writer = SummaryWriter(log_dir = f'../runs/{experiment_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e98894df-598d-4ad2-88e0-8c84af4904bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(mapper.parameters(), lr=learning_rate)\n",
    "# multioptimizer = optim.Adam(list(mapper.parameters()) + list(prompt.parameters()), lr=learning_rate)\n",
    "optimizer = optim.Adam(list(mapper.parameters()) + list(reverseMapper.parameters()), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rl_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ad53cb6-4a59-4fa2-93eb-b1b2a4872e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_hparams(\n",
    "    {\n",
    "        \"lr\": learning_rate,\n",
    "        \"data_type\": exp_type,\n",
    "        \"algo\": algo,\n",
    "        \"gamma\": gamma,\n",
    "        \"temperature\": temperature,\n",
    "        \"scale_rate\": alpha,\n",
    "        \"prompt_length\": prompt_len,\n",
    "    },\n",
    "    {},\n",
    "    run_name=experiment_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02a2fb2e-0a55-4525-8c72-51543e1724d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating with REINFORCE LOSS\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 70\u001b[0m\n\u001b[1;32m     66\u001b[0m     ce_loss \u001b[38;5;241m=\u001b[39m ce_loss\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     68\u001b[0m rl_loss \u001b[38;5;241m=\u001b[39m Reinforce_Loss(translate_logits[:,\u001b[38;5;241m1\u001b[39m:], translated_text_tokens[:,\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mdetach(), ce_loss, alpha\u001b[38;5;241m=\u001b[39malpha, gamma\u001b[38;5;241m=\u001b[39mgamma, temperature\u001b[38;5;241m=\u001b[39mtemperature)\n\u001b[0;32m---> 70\u001b[0m \u001b[43mrl_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# writer.add_scalar(\"training_rl\", rl_loss.item(), epoch*len(midi_loader)+i)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Log the losses\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vqvae/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vqvae/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "if 'base' in algo:\n",
    "    print(\"Calculating with BASE LOSS\\n\")\n",
    "elif 'rl' in algo:\n",
    "    print(\"Calculating with REINFORCE LOSS\\n\")\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    mapper.train()\n",
    "    # mapper.eval()\n",
    "    for i, midi in enumerate(midi_loader):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        # multioptimizer.zero_grad()\n",
    "        \n",
    "        # midi_one_hot shape -> [batch_size, seq_len, feature_dim]\n",
    "        ground_truth_tokens = midi[\"input_ids\"].to(device)\n",
    "        one_hot_tokens = F.one_hot(ground_truth_tokens, num_classes=midi_vocab_len).float().to(device)\n",
    "        batch_len = one_hot_tokens.size(0)\n",
    "        \n",
    "        # break\n",
    "        # Logits are to be compared with the next ground truth tokens\n",
    "        ground_truth_tokens = ground_truth_tokens[:,1:]\n",
    "        inputs_feature_vector = mapper(one_hot_tokens)\n",
    "        \n",
    "        # Add prompt to input\n",
    "        # prompt_feature_vector = prompt(prompt_inputs)\n",
    "        # prompt_feature_vector = prompt_feature_vector.unsqueeze(0).repeat(batch_len, 1, 1)\n",
    "        # inputs_feature_vector = torch.cat((prompt_feature_vector, mapped_feature_vector), dim=1)\n",
    "\n",
    "        # Map tokens and get ground truth from LLM\n",
    "        translated_feature_vector, translated_logits, translated_text_tokens = translate(inputs_feature_vector, embeddings.detach(), temperature=temperature)\n",
    "        # translated_feature_vector, translated_logits, translated_text_tokens = translate(inputs_feature_vector, embeddings.detach(), temperature=temperature)\n",
    "\n",
    "        # Calculate Representation of Last Layer in LLM\n",
    "        final_layer_fv = generate_next_token_predictions_withfv(translated_feature_vector)\n",
    "        # final_layer_fv = generate_next_token_predictions(translated_text_tokens.long()).to(device)\n",
    "\n",
    "        # Calculate Logits with mapper function\n",
    "        # final_layer_fv = F.normalize(final_layer_fv, dim=-1)\n",
    "        # mapper_embeds = F.normalize(mapper.mapper.weight, dim=0)\n",
    "        logits = torch.matmul(final_layer_fv, reverseMapper.mapper.weight)\n",
    "        # logits = logits[:,prompt_len:-1]\n",
    "        logits = logits[:,:-1]\n",
    "        logits_ = logits.reshape(-1, midi_vocab_len)\n",
    "        ground_truth_tokens = ground_truth_tokens.reshape(-1)        \n",
    "        ce_loss = criterion(logits_, ground_truth_tokens)\n",
    "        ce_loss.backward()\n",
    "        optimizer.step()\n",
    "        if 'base' in algo:\n",
    "            # ce_loss.backward()\n",
    "            # optimizer.step()\n",
    "            writer.add_scalar(\"training/cross_entropy_base\", ce_loss.item(), epoch*len(midi_loader)+i)\n",
    "            if i%50==0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {i}, CE Loss: {ce_loss.mean().item()}\")\n",
    "        # RL Loss\n",
    "        if 'rl' in algo:\n",
    "            optimizer.zero_grad()\n",
    "            # action_logits = torch.matmul(mapped_feature_vector, embeddings.T.detach())\n",
    "            translated_feature_vector, translate_logits, translated_text_tokens = translate(inputs_feature_vector, embeddings.detach(), temperature=temperature)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                final_layer_fv = generate_next_token_predictions_withfv(translated_feature_vector)\n",
    "                logits = torch.matmul(final_layer_fv, reverseMapper.mapper.weight)\n",
    "                logits = logits[:,prompt_len:-1]\n",
    "                logits_ = logits.reshape(-1, midi_vocab_len)\n",
    "                ce_loss = rl_criterion(logits_, ground_truth_tokens)\n",
    "                ce_loss = ce_loss.reshape(-1, logits.size(1))\n",
    "                \n",
    "            rl_loss = Reinforce_Loss(translate_logits[:,1:], translated_text_tokens[:,1:].detach(), ce_loss, alpha=alpha, gamma=gamma, temperature=temperature)\n",
    "            \n",
    "            rl_loss.backward()\n",
    "            optimizer.step()\n",
    "            # writer.add_scalar(\"training_rl\", rl_loss.item(), epoch*len(midi_loader)+i)\n",
    "            # Log the losses\n",
    "            writer.add_scalars(\n",
    "                \"training\",\n",
    "                {\n",
    "                    \"rl_loss\": rl_loss.item(),\n",
    "                    \"cross_entropy_rl\": ce_loss.mean().item(),\n",
    "                },\n",
    "                epoch * len(midi_loader) + i\n",
    "            )\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {i}, CE Loss: {ce_loss.mean().item()}, RL Loss: {rl_loss.item()}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed.\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81d0c8bd-bd0f-4d58-a50a-98a0ac8b6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f\"../models/{experiment_name}\").mkdir(parents=True, exist_ok=True)\n",
    "torch.save(mapper.state_dict(), f\"../models/{experiment_name}/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4be09272-d867-42ae-bcaf-672af7863786",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fdea4c-8ba1-4882-9418-12623e545d57",
   "metadata": {},
   "source": [
    "## Compression Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f98b1-593d-44db-a55c-1b1445492aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper.load_state_dict(torch.load(f\"../models/{experiment_name}/model.pt\"))\n",
    "mapper.eval()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa3029-efba-41d1-acb3-077a99c64e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compression(total_loss, tokens, compress_bits, file_size):\n",
    "    token_len = len(tokens.ids)\n",
    "    return (total_loss / token_len) / math.log2(midi_vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c3cd4-0486-47f7-9b60-4d37d5ce580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "for i, midi in enumerate(val_loader):\n",
    "\n",
    "    # midi_one_hot shape -> [batch_size, seq_len, feature_dim]\n",
    "    ground_truth_tokens = midi[\"input_ids\"].to(device)\n",
    "    one_hot_tokens = F.one_hot(ground_truth_tokens, num_classes=midi_vocab_len).float().to(device)\n",
    "    batch_len = one_hot_tokens.size(0)\n",
    "    \n",
    "    # break\n",
    "    # Logits are to be compared with the next ground truth tokens\n",
    "    ground_truth_tokens = ground_truth_tokens[:,1:]\n",
    "    inputs_feature_vector = mapper(one_hot_tokens)\n",
    "    \n",
    "    # Add prompt to input\n",
    "    # prompt_feature_vector = prompt(prompt_inputs)\n",
    "    # prompt_feature_vector = prompt_feature_vector.unsqueeze(0).repeat(batch_len, 1, 1)\n",
    "    # inputs_feature_vector = torch.cat((prompt_feature_vector, mapped_feature_vector), dim=1)\n",
    "\n",
    "    # Map tokens and get ground truth from LLM\n",
    "    translated_feature_vector, translated_logits, translated_text_tokens = translate(inputs_feature_vector, embeddings.detach(), temperature=temperature)\n",
    "    # translated_feature_vector, translated_logits, translated_text_tokens = translate(inputs_feature_vector, embeddings.detach(), temperature=temperature)\n",
    "\n",
    "    # Calculate Representation of Last Layer in LLM\n",
    "    final_layer_fv = generate_next_token_predictions_withfv(translated_feature_vector)\n",
    "    # final_layer_fv = generate_next_token_predictions(translated_text_tokens.long()).to(device)\n",
    "          \n",
    "    # Calculate Logits with mapper function\n",
    "    # final_layer_fv = F.normalize(final_layer_fv, dim=-1)\n",
    "    # mapper_embeds = F.normalize(mapper.mapper.weight, dim=0)\n",
    "    logits = torch.matmul(final_layer_fv, mapper.mapper.weight)\n",
    "    # logits = logits[:,prompt_len:-1]\n",
    "    logits = logits[:,:-1]\n",
    "    logits_ = logits.reshape(-1, midi_vocab_len)\n",
    "    ground_truth_tokens = ground_truth_tokens.reshape(-1)\n",
    "\n",
    "    loss = criterion(logits_, ground_truth_tokens)\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "print(\"testing loss avg:\", total_loss / val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5361a9-d577-4516-9e2b-3e8394ee51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_path = \"../data/midi/Maestro/2009/MIDI-Unprocessed_15_R1_2009_03-06_ORIG_MID--AUDIO_15_R1_2009_15_R1_2009_06_WAV.midi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c601b0d8-19e4-4513-b9c2-6632d514a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f352b-d849-4bb1-b36a-86feb7347455",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi = MidiFile(midi_path)\n",
    "tokens = midi_tokenizer(midi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc2b69-8449-4ffb-a66c-98924fb15c56",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "compress_bits = total_loss / math.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849f395-f716-4cb8-a40c-dd62579b134f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e00dd3-b32d-44a1-aca6-dbe7f980e156",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(tokens.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f81421-fb52-49fb-b167-858c637ba5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7afb033-a009-482e-bb60-4cfa5f8b07c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# midi_one_hot shape -> [batch_size, seq_len, feature_dim]\n",
    "total_loss = 0\n",
    "for i in range(0, len(tokens.ids), length//2):\n",
    "    if i + length >= len(tokens.ids):\n",
    "        break\n",
    "        ground_truth_tokens = torch.tensor(tokens.ids[i:]).to(device)\n",
    "    else:\n",
    "        ground_truth_tokens = torch.tensor(tokens.ids[i:i+length]).to(device)\n",
    "    ground_truth_tokens = ground_truth_tokens.unsqueeze(dim=0)\n",
    "    \n",
    "    if i == 0:  \n",
    "        one_hot_tokens = F.one_hot(ground_truth_tokens, num_classes=midi_vocab_len).float().to(device)\n",
    "    else:\n",
    "        one_hot_tokens = F.one_hot(ground_truth_tokens[:,length//2:], num_classes=midi_vocab_len).float().to(device)\n",
    "        one_hot_tokens = torch.cat((logits[:,length//2:], one_hot_tokens), dim=1)\n",
    "    \n",
    "    # break\n",
    "    # Logits are to be compared with the next ground truth tokens\n",
    "    ground_truth_tokens = ground_truth_tokens[:,1:]\n",
    "\n",
    "    \n",
    "    inputs_feature_vector = mapper(one_hot_tokens)\n",
    "\n",
    "    # if i!=0:\n",
    "    #     inputs_feature_vector = torch.cat((logits[:,length//2], inputs_feature_vector), dim=1)\n",
    "\n",
    "    # print(inputs_feature_vector.shape)\n",
    "    # input('stop')\n",
    "    \n",
    "    # Add prompt to input\n",
    "    # prompt_feature_vector = prompt(prompt_inputs)\n",
    "    # prompt_feature_vector = prompt_feature_vector.unsqueeze(0).repeat(batch_len, 1, 1)\n",
    "    # inputs_feature_vector = torch.cat((prompt_feature_vector, inputs_feature_vector), dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Map tokens and get ground truth from LLM\n",
    "        translated_feature_vector, translated_logits, translated_text_tokens = translate(inputs_feature_vector, embeddings.detach(), temperature=temperature)\n",
    "        # translated_feature_vector, translated_logits, translated_text_tokens = translate(inputs_feature_vector, embeddings.detach(), temperature=temperature)\n",
    "        \n",
    "        # Calculate Representation of Last Layer in LLM\n",
    "        final_layer_fv = generate_next_token_predictions_withfv(translated_feature_vector)\n",
    "        # final_layer_fv = generate_next_token_predictions(translated_text_tokens.long()).to(device)\n",
    "              \n",
    "        # Calculate Logits with mapper function\n",
    "        # final_layer_fv = F.normalize(final_layer_fv, dim=-1)\n",
    "        # mapper_embeds = F.normalize(mapper.mapper.weight, dim=0)\n",
    "        logits = torch.matmul(final_layer_fv, mapper.mapper.weight)\n",
    "        # logits = logits[:,prompt_len:-1]\n",
    "        logits_ = logits[:,:-1].reshape(-1, midi_vocab_len)\n",
    "        ground_truth_tokens = ground_truth_tokens.reshape(-1)\n",
    "\n",
    "    if i==0:\n",
    "        loss = criterion(logits_, ground_truth_tokens)\n",
    "        print(f\"loss from tokens 0 to {i+length} = {loss.item()}\")\n",
    "        total_loss += loss.item()*length\n",
    "    else:\n",
    "        loss = criterion(logits_[length//2:], ground_truth_tokens[length//2:])\n",
    "        print(f\"loss from tokens {i+length//2} to {i+length} = {loss.item()}\")\n",
    "        total_loss += loss.item()*length/2\n",
    "\n",
    "print(\"testing loss:\", total_loss/len(tokens.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886a628-87ab-4dd3-873e-53bbce113c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_compression(total_loss, tokens, compress_bits, file_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bf796-81cf-413c-ac85-b1bd1a377479",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_midi_file_size(file_path):\n",
    "    try:\n",
    "        # Open the file in binary mode and read its contents\n",
    "        with open(file_path, 'rb') as file:\n",
    "            file_contents = file.read()\n",
    "            # Return the size of the file\n",
    "            return len(file_contents)\n",
    "    except FileNotFoundError:\n",
    "        return \"File not found.\"\n",
    "\n",
    "# Example usage\n",
    "file_size = get_midi_file_size(midi_path)\n",
    "# print(f\"The MIDI file size is: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7fab5-1149-4ea2-b957-0c60f6f739a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db9eddb-aed9-41b1-9fb9-11ff900b822d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqvae",
   "language": "python",
   "name": "vqvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
