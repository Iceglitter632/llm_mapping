{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366e69dd-6839-42c1-92fb-8a9766e45b19",
   "metadata": {},
   "source": [
    "## Import Needed Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1d2f0-21d1-4c06-b4ea-8b5708205a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be9b7c-2991-4007-8890-2ede65e43c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display, display_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864141c-f356-49ec-a439-a855160f7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8057b-5c43-4d55-9641-caf91f0de4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f992d-a311-4d18-b077-89fd103ebcce",
   "metadata": {},
   "source": [
    "## VQGAN as Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0559534-03de-4029-aef6-a994f39a3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqgan_jax.modeling_flax_vqgan import VQModel\n",
    "\n",
    "# Load the pre-trained VQGAN model and its processor\n",
    "checkpoint = \"dalle-mini/vqgan_imagenet_f16_16384\"\n",
    "vqmodel = VQModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e2b06-9f22-4853-8055-5879bed2e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return Image.open(io.BytesIO(resp.content))\n",
    "\n",
    "def preprocess_vqgan(x):\n",
    "  x = 2.*x - 1.\n",
    "  return x\n",
    "\n",
    "def custom_to_pil(x):\n",
    "  x = np.clip(x, -1., 1.)\n",
    "  x = (x + 1.)/2.\n",
    "  x = (255*x).astype(np.uint8)\n",
    "  x = Image.fromarray(x)\n",
    "  if not x.mode == \"RGB\":\n",
    "    x = x.convert(\"RGB\")\n",
    "  return x\n",
    "\n",
    "def preprocess(img, target_image_size=256,):\n",
    "    s = min(img.size)\n",
    "    \n",
    "    if s < target_image_size:\n",
    "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "        \n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "    return img.permute(0, 2, 3, 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab439a5-04bd-4801-8a5a-503ed38ce773",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vocab_len = vqmodel.config.n_embed\n",
    "print(\"image_vocab_len:\", image_vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8021919-8047-4232-bc01-6a971b315a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://heibox.uni-heidelberg.de/f/7bb608381aae4539ba7a/?dl=1'\n",
    "size=256\n",
    "image = download_image(url)\n",
    "image = preprocess(image, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8792470-5521-4f88-bf30-b9d4615b879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_to_pil(preprocess_vqgan(image[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7cb35-e3d1-4709-bf60-bfcf106aee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_states, indices = vqmodel.encode(image)\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7397b-f8b9-4a96-a598-e0e3dbc84cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = vqmodel.decode(quant_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea419ea4-4bb4-425f-beb9-c7be0a2d3c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_to_pil(preprocess_vqgan(np.asarray(rec[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39b2ce-583c-4c7f-bbcd-63c6092f4199",
   "metadata": {},
   "source": [
    "## Text LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b9076-f123-4ad3-9d3b-a4fe1f6af9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# llm = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model = LlamaForCausalLM.from_pretrained(llm)\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9b5f8-0a99-4d08-9a29-7ea1456b5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "llm = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(llm)\n",
    "llm_tokenizer = GPT2Tokenizer.from_pretrained(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205e7cb-7b7d-4280-bc7e-4f97f3de6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.lm_head.weight\n",
    "# embedding_matrix = model.transformer.wte.weight\n",
    "llm_feature_dim = model.config.hidden_size\n",
    "llm_vocab_len = model.config.vocab_size\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469ea76-4126-463c-84b4-60ae60a640e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = embeddings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8413d-4467-4f4a-b1f4-ff89f690fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gpt2 feature dim length:\", llm_feature_dim)\n",
    "print(\"gpt2 vocabulary length:\", llm_vocab_len)\n",
    "print(\"gpt2 embedding shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e68f45-f220-4f68-99b4-2ec4f4e93803",
   "metadata": {},
   "source": [
    "## Mapper Network\n",
    "\n",
    "map some modality to text token's feature dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d24430-98fe-4cab-8d5f-387d00094e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74b816-7d75-4b25-91ec-7307266912af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenMapper(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.mapper = nn.Linear(input_dim, output_dim)\n",
    "        self.mapper.to(device)\n",
    "\n",
    "    def forward(self, one_hot_token):\n",
    "        return self.mapper(one_hot_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ca007-4c83-4196-b5a1-ad7bf2c2a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapper\n",
    "# mapper maps vocabulary_size of target modality to feature_dimension size of llm\n",
    "# mapper = TokenMapper(midi_vocab_len, llm_feature_dim, device=device)\n",
    "mapper = TokenMapper(image_vocab_len, llm_feature_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb20d95-9c20-4d00-92c1-531ab718190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afdbe0-2789-4c1a-b2b4-792cc84fd05d",
   "metadata": {},
   "source": [
    "## Generate Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298cf65-3f3a-41a6-8edb-f3c1cba88ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token_predictions(token_sequences):\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=token_sequences, output_hidden_states=True)\n",
    "    \n",
    "    return outputs.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37290d-4b1c-4c74-8f28-378382b80b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(batch_feature_vectors, embeddings):\n",
    "    batch_size, seq_len, embedding_dim = batch_feature_vectors.shape\n",
    "    closest_tokens = torch.zeros((batch_size, seq_len), dtype=torch.long)\n",
    "\n",
    "    # Normalize the embedding matrix\n",
    "    embedding_matrix_norm = F.normalize(embeddings, dim=1)\n",
    "\n",
    "    closest_tokens = torch.zeros((batch_size, seq_len), dtype=torch.long).to(device)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Normalize the feature vectors for the i-th sample in the batch\n",
    "        feature_vectors_norm = F.normalize(batch_feature_vectors[i], dim=1)\n",
    "\n",
    "        # Compute cosine similarity for the entire sequence at once\n",
    "        cosine_similarities = torch.matmul(feature_vectors_norm, embedding_matrix_norm.T)\n",
    "\n",
    "        # Find the token with the highest similarity for each feature vector\n",
    "        closest_tokens[i] = torch.argmax(cosine_similarities, dim=1)\n",
    "\n",
    "    return closest_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec8daa-a7c4-461d-8cfd-391a4d0287d9",
   "metadata": {},
   "source": [
    "## Get Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c1e6e-dbb0-43cf-84d1-a43bbc6e976d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139ed24-d020-461d-992e-c029aea50089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((128, 128)),  # Resize to a fixed size; adjust as needed\n",
    "#     transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize (mean, std) for each color channel\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900e842-7e02-44bc-aafd-7f989c546f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "\n",
    "def resize_and_crop(img):\n",
    "    # Resize while maintaining aspect ratio and center crop\n",
    "    s = min(img.size)\n",
    "    r = image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [image_size])\n",
    "    return img\n",
    "\n",
    "def modified_map_pixels(img):\n",
    "    # Add a batch dimension, apply map_pixels, and then remove the batch dimension\n",
    "    img = img.unsqueeze(0)\n",
    "    img = map_pixels(img)\n",
    "    return img.squeeze(0)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.Lambda(resize_and_crop),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(modified_map_pixels)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a29062-3951-4668-b8a8-5f088e84a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'path/to/lsun' with the actual path to your LSUN dataset\n",
    "dataset_path = '../data/lsun'\n",
    "\n",
    "lsun_dataset = datasets.LSUN(root=dataset_path, classes=['classroom_train'], transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c28ed-748a-4109-95e8-d1391e6523c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5  # Adjust based on your memory availability and requirements\n",
    "lsun_loader = DataLoader(lsun_dataset, batch_size=batch_size, shuffle=TrueS)\n",
    "print('dataset size:',len(lsun_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbde88c-37ae-41c3-90e3-aa5233d237c7",
   "metadata": {},
   "source": [
    "## Reinforce Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16ec45-f652-48da-8132-0c5aff6ab98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reinforce_Loss(logits, translated, loss, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Calculate the REINFORCE loss for sequence prediction.\n",
    "\n",
    "    :param logits: Logits from the model, shape [batch_size, seq_len, vocab_size].\n",
    "    :param targets: Ground truth sequence, shape [batch_size, seq_len].\n",
    "    :param rewards: Reward for each step in the sequence, shape [batch_size, seq_len].\n",
    "    :param gamma: Discount factor for future rewards.\n",
    "    :return: The REINFORCE loss (to be maximized).\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = logits.shape\n",
    "\n",
    "    # shape = [batch_size, seq_len, llm_vocab_len]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    log_probs_targets = log_probs.gather(2, translated.unsqueeze(2)).squeeze(2)\n",
    "    \n",
    "    # Create a discount matrix\n",
    "    discount_matrix = torch.zeros((seq_len, seq_len)).to(device)\n",
    "\n",
    "    # Fill the matrix according to the given pattern\n",
    "    for i in range(seq_len):\n",
    "        for j in range(i, seq_len):\n",
    "            discount_matrix[i, j] = gamma ** (j - i)\n",
    "\n",
    "    normalize_factor = discount_matrix.sum(dim=1)\n",
    "    \n",
    "    # Calculate discounted rewards\n",
    "    discounted_loss = loss.unsqueeze(1) * discount_matrix\n",
    "    cumulative_loss = discounted_loss.sum(dim=-1) / normalize_factor\n",
    "    \n",
    "    # Calculate loss\n",
    "    total_loss = torch.sum(log_probs_targets * cumulative_loss.detach()) / batch_size / seq_len\n",
    "    # total_loss = -torch.sum(log_probs_targets * cumulative_loss) / batch_size / seq_len\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4a481-89b6-4f0f-87b2-49b6bbeffd95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "244bb1b9-75a5-4c18-953b-1b3434bca223",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b3605-366c-4271-817d-b69ab712e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "learning_rate = 1e-5\n",
    "epochs = 1\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ecacfe-73b7-4d80-b59c-7cbcb01d60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"base_test\"\n",
    "exp_type = \"image\"\n",
    "name = \"vqgan\"\n",
    "experiment_name = f\"{exp_type}/{experiment}/{name}/model={llm}_lr={learning_rate}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b859448-9984-477d-a96c-a2b424627a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Create a SummaryWriter instance (logs will be saved in 'runs' folder)\n",
    "writer = SummaryWriter(log_dir = f'../runs/{experiment_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98894df-598d-4ad2-88e0-8c84af4904bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(mapper.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "rl_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2fb2e-0a55-4525-8c72-51543e1724d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    mapper.train()\n",
    "    # mapper.eval()\n",
    "    for i, (images, _) in enumerate(lsun_loader):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # for vqgan\n",
    "        images = images.permute(0, 2, 3, 1)\n",
    "        _, ground_truth_tokens = vqmodel.encode(images)\n",
    "        ground_truth_tokens = torch.from_dlpack(ground_truth_tokens)\n",
    "        ground_truth_tokens = ground_truth_tokens.to(torch.int64).to(device)\n",
    "        one_hot_tokens = F.one_hot(ground_truth_tokens, num_classes=image_vocab_len).float()\n",
    "\n",
    "        # Logits are to be compared with the next ground truth tokens\n",
    "        ground_truth_tokens = ground_truth_tokens[:,1:]\n",
    "\n",
    "        # Map tokens and get ground truth from LLM\n",
    "        mapped_feature_vector = mapper(one_hot_tokens)\n",
    "\n",
    "        translated_text_tokens = translate(mapped_feature_vector, embeddings)\n",
    "        # Calculate Representation of Last Layer in LLM\n",
    "        final_layer_fv = generate_next_token_predictions(translated_text_tokens)\n",
    "\n",
    "        # Calculate Logits with mapper function\n",
    "        logits = torch.matmul(final_layer_fv, mapper.mapper.weight)\n",
    "        logits = logits[:,:-1]\n",
    "        logits_ = logits.reshape(-1, image_vocab_len)\n",
    "        ground_truth_tokens = ground_truth_tokens.reshape(-1)\n",
    "        \n",
    "        loss = criterion(logits_, ground_truth_tokens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if 'base' in experiment:\n",
    "            writer.add_scalar(\"training/cross_entropy\", loss.item(), epoch*len(lsun_loader)+i)\n",
    "        # RL Loss\n",
    "        if 'rl' in experiment:\n",
    "            \n",
    "            action_logits = torch.matmul(mapped_feature_vector, embeddings.T)\n",
    "            with torch.no_grad():\n",
    "                ce_loss = rl_criterion(logits_, ground_truth_tokens)\n",
    "            ground_truth_tokens = ground_truth_tokens.reshape(batch_size, -1)\n",
    "            ce_loss = ce_loss.reshape(batch_size, -1)\n",
    "    \n",
    "            loss = Reinforce_Loss(action_logits, translated_text_tokens, ce_loss)\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log the losses\n",
    "            writer.add_scalars(\n",
    "                \"training\",\n",
    "                {\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"cross_entropy\": ce_loss.mean().item(),\n",
    "                },\n",
    "                epoch * len(lsun_loader) + i\n",
    "            )\n",
    "            \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {i}, Loss: {loss.item()}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed.\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0c8bd-bd0f-4d58-a50a-98a0ac8b6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f\"../models/{experiment_name}\").mkdir(parents=True, exist_ok=True)\n",
    "torch.save(mapper.state_dict(), f\"../models/{experiment_name}/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be09272-d867-42ae-bcaf-672af7863786",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa859ab-774b-432f-a564-c0795db6bbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2448167-be48-4f48-8aaa-c1d994ba3f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqvae",
   "language": "python",
   "name": "vqvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
